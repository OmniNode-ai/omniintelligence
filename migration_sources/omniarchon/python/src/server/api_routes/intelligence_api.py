"""
Intelligence API Routes

Provides endpoints for accessing intelligence documents and correlation data
generated by Enhanced Intelligence Hooks across repositories.

This module acts as a thin API layer over the intelligence_service.
"""

import logging
import time
from typing import Optional

from fastapi import APIRouter, HTTPException, Query
from server.services.enhanced_correlation_processor import (
    get_enhanced_correlation_processor,
)

# Import models and functions from the shared service
from server.services.intelligence_service import (
    IntelligenceResponse,
    IntelligenceStats,
    get_active_repositories,
    get_intelligence_documents,
    get_intelligence_stats,
)

# Set up logging
logger = logging.getLogger(__name__)

# Create router with prefix
router = APIRouter(prefix="/api/intelligence", tags=["intelligence"])


@router.get("/documents", response_model=IntelligenceResponse)
async def get_intelligence_documents_endpoint(
    repository: Optional[str] = Query(None, description="Filter by repository"),
    time_range: str = Query("24h", description="Time range: 1h, 6h, 24h, 72h, 7d"),
    limit: int = Query(50, ge=1, le=1000, description="Maximum number of documents"),
    offset: int = Query(0, ge=0, description="Offset for pagination"),
):
    """
    Retrieve intelligence documents with optional filtering.

    Intelligence documents contain analysis of code changes including:
    - Diff analysis (lines changed, files modified)
    - Cross-repository correlation analysis
    - Security pattern detection
    - Breaking change detection
    """
    from ..utils.correlation_logging import api_logger

    # Generate correlation ID and start request logging
    correlation_id = api_logger.generate_correlation_id()

    with api_logger.correlation_context(correlation_id):
        request_start_time = time.time()

        api_logger.log_api_request(
            "GET /api/intelligence/documents",
            {
                "repository": repository,
                "time_range": time_range,
                "limit": limit,
                "offset": offset,
            },
            correlation_id,
        )

        try:
            api_logger.log_processing_start(
                "document_retrieval",
                {
                    "query_parameters": {
                        "repository": repository,
                        "time_range": time_range,
                        "limit": limit,
                        "offset": offset,
                    }
                },
            )

            # Use the shared service function with enhanced logging
            api_logger.log_debug(
                "calling_intelligence_service",
                {
                    "service_function": "get_intelligence_documents",
                    "parameters": {
                        "repository": repository,
                        "time_range": time_range,
                        "limit": limit,
                        "offset": offset,
                    },
                },
            )

            result = await get_intelligence_documents(
                repository=repository, time_range=time_range, limit=limit, offset=offset
            )

            api_logger.log_processing_complete(
                "document_retrieval",
                {
                    "documents_returned": (
                        len(result.documents) if result.documents else 0
                    ),
                    "total_found": result.total_count,
                    "has_correlations": (
                        sum(
                            1
                            for doc in result.documents
                            if (
                                doc.intelligence_data.correlation_analysis
                                and (
                                    len(
                                        doc.intelligence_data.correlation_analysis.temporal_correlations
                                    )
                                    > 0
                                    or len(
                                        doc.intelligence_data.correlation_analysis.semantic_correlations
                                    )
                                    > 0
                                )
                            )
                        )
                        if result.documents
                        else 0
                    ),
                    "empty_correlations": (
                        sum(
                            1
                            for doc in result.documents
                            if (
                                not doc.intelligence_data.correlation_analysis
                                or (
                                    len(
                                        doc.intelligence_data.correlation_analysis.temporal_correlations
                                    )
                                    == 0
                                    and len(
                                        doc.intelligence_data.correlation_analysis.semantic_correlations
                                    )
                                    == 0
                                )
                            )
                        )
                        if result.documents
                        else 0
                    ),
                },
            )

            # Log response metrics
            response_duration = (time.time() - request_start_time) * 1000  # ms
            api_logger.log_api_response(
                "GET /api/intelligence/documents",
                200,
                {
                    "documents_count": len(result.documents) if result.documents else 0,
                    "total_count": result.total_count,
                },
                response_duration,
                correlation_id,
            )

            return result

        except Exception as e:
            response_duration = (time.time() - request_start_time) * 1000  # ms

            api_logger.log_processing_error(
                "document_retrieval",
                e,
                {
                    "query_parameters": {
                        "repository": repository,
                        "time_range": time_range,
                        "limit": limit,
                        "offset": offset,
                    }
                },
            )

            api_logger.log_api_response(
                "GET /api/intelligence/documents",
                500,
                {"error": str(e)},
                response_duration,
                correlation_id,
            )

            logger.error(f"Error fetching intelligence documents: {e}")
            raise HTTPException(
                status_code=500,
                detail=f"Failed to fetch intelligence documents: {e!s}",
            )


@router.get("/stats", response_model=IntelligenceStats)
async def get_intelligence_stats_endpoint(
    repository: Optional[str] = Query(None, description="Filter by repository"),
    time_range: str = Query("24h", description="Time range: 1h, 6h, 24h, 72h, 7d"),
):
    """
    Get aggregated statistics about intelligence activity.

    Returns metrics like:
    - Total number of changes analyzed
    - Total correlations detected
    - Average correlation strength
    - Number of breaking changes
    - Number of active repositories
    """
    try:
        # Use the shared service function
        return await get_intelligence_stats(
            repository=repository, time_range=time_range
        )

    except Exception as e:
        logger.error(f"Error calculating intelligence stats: {e}")
        raise HTTPException(
            status_code=500, detail=f"Failed to calculate intelligence stats: {e!s}"
        )


@router.get("/repositories")
async def get_active_repositories_endpoint():
    """
    Get list of repositories that have generated intelligence data.
    """
    try:
        # Use the shared service function
        repositories = await get_active_repositories()
        return {"repositories": repositories}

    except Exception as e:
        logger.error(f"Error fetching repositories: {e}")
        raise HTTPException(
            status_code=500, detail=f"Failed to fetch repositories: {e!s}"
        )


@router.post("/generate-correlations")
async def generate_correlations_endpoint():
    """
    Generate correlations for documents with empty correlation arrays.

    This endpoint triggers the automated correlation generation system that:
    - Identifies documents with empty temporal_correlations, semantic_correlations, and breaking_changes
    - Analyzes relationships between commits across repositories
    - Generates temporal correlations (time-based relationships)
    - Generates semantic correlations (content-based similarities)
    - Detects potential breaking changes
    - Updates the documents with the generated correlation data

    Returns:
        JSON with processing results and statistics
    """
    from ..utils.correlation_logging import api_logger

    correlation_id = api_logger.generate_correlation_id()

    with api_logger.correlation_context(correlation_id):
        request_start_time = time.time()

        api_logger.log_api_request(
            "POST /api/intelligence/generate-correlations",
            {"operation": "generate_correlations", "mode": "empty_documents_only"},
            correlation_id,
        )

        try:
            api_logger.log_processing_start(
                "correlation_generation",
                {
                    "operation_type": "generate_empty_correlations",
                    "triggered_via": "api_endpoint",
                },
            )

            logger.info("üöÄ Starting enhanced correlation generation via API")
            processor = get_enhanced_correlation_processor()
            queued_count = await processor.queue_documents_with_empty_correlations()
            await processor.process_batch()
            results = {
                "processed_documents": queued_count,
                "message": f"Enhanced correlation processing initiated for {queued_count} documents",
            }

            api_logger.log_processing_complete(
                "correlation_generation",
                {
                    "processed_documents": results.get("processed_documents", 0),
                    "total_correlations_generated": results.get(
                        "total_correlations_generated", 0
                    ),
                    "temporal_correlations": results.get("temporal_correlations", 0),
                    "semantic_correlations": results.get("semantic_correlations", 0),
                    "breaking_changes": results.get("breaking_changes", 0),
                    "processing_errors": results.get("processing_errors", 0),
                },
            )

            response_duration = (time.time() - request_start_time) * 1000  # ms
            api_logger.log_api_response(
                "POST /api/intelligence/generate-correlations",
                200,
                {
                    "documents_processed": results.get("processed_documents", 0),
                    "correlations_generated": results.get(
                        "total_correlations_generated", 0
                    ),
                },
                response_duration,
                correlation_id,
            )

            logger.info(
                f"‚úÖ Correlation generation completed: {results.get('processed_documents', 0)} documents updated"
            )
            return {
                "success": True,
                "message": "Correlation generation completed successfully",
                "results": results,
            }

        except Exception as e:
            response_duration = (time.time() - request_start_time) * 1000  # ms

            api_logger.log_processing_error(
                "correlation_generation",
                e,
                {
                    "operation_type": "generate_empty_correlations",
                    "triggered_via": "api_endpoint",
                },
            )

            api_logger.log_api_response(
                "POST /api/intelligence/generate-correlations",
                500,
                {"error": str(e)},
                response_duration,
                correlation_id,
            )

            logger.error(f"‚ùå Error generating correlations: {e}")
            raise HTTPException(
                status_code=500, detail=f"Failed to generate correlations: {e!s}"
            )


@router.post("/force-regenerate-correlations")
async def force_regenerate_correlations_endpoint():
    """
    Force regenerate ALL correlations, clearing old data including 100% values.

    This endpoint triggers complete correlation regeneration that:
    - Processes ALL documents (not just empty ones)
    - Clears existing correlation data (including old 100% values)
    - Uses improved intelligent analysis system
    - Generates realistic correlations with file-level analysis
    - Provides meaningful semantic insights with common concepts

    Use this to fix dashboards showing 100% correlations.

    Returns:
        JSON with comprehensive processing results and statistics
    """
    try:
        logger.info("üîÑ Starting FORCE regeneration of all correlations via API")

        from ..services.correlation_generator import AutomatedCorrelationGenerator

        generator = AutomatedCorrelationGenerator()
        results = await generator.force_regenerate_all_correlations()

        logger.info(
            f"‚úÖ Force regeneration completed: {results.get('processed_documents', 0)} documents updated, "
            f"{results.get('cleared_documents', 0)} cleared"
        )
        return {
            "success": True,
            "message": "Force correlation regeneration completed successfully",
            "results": results,
            "improvements": [
                "Cleared old 100% correlation values",
                "Applied intelligent content analysis",
                "Generated realistic correlation percentages",
                "Added semantic concept extraction",
                "Improved file-level correlation insights",
            ],
        }

    except Exception as e:
        logger.error(f"‚ùå Error force regenerating correlations: {e}")
        raise HTTPException(
            status_code=500, detail=f"Failed to force regenerate correlations: {e!s}"
        )
