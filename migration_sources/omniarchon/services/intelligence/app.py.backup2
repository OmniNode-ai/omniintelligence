"""
Archon Intelligence Service - FastAPI Application

Entity extraction and semantic analysis service for knowledge graph population.
Based on omnibase_3 intelligence patterns with Archon-specific enhancements.
"""

import asyncio
import hashlib
import logging
import os
import sys
import time
from contextlib import asynccontextmanager
from datetime import datetime, timezone
from pathlib import Path
from typing import List, Optional

# Add python lib to path for config validator
sys.path.insert(0, os.path.join(os.path.dirname(__file__), "../.."))
# Add langextract service to path for code relationship detection
# In Docker: /app/langextract/, In local: ../langextract
langextract_paths = [
    "/app/langextract",  # Docker container path
    os.path.join(os.path.dirname(__file__), "../langextract"),  # Local development path
]
for path in langextract_paths:
    if os.path.exists(path):
        sys.path.insert(0, path)
        break

import httpx
from extractors.enhanced_extractor import EnhancedEntityExtractor

# Import CodeRelationshipDetector (required for file import extraction)
try:
    from analysis.code_relationship_detector import CodeRelationshipDetector

    CODE_RELATIONSHIP_DETECTOR_AVAILABLE = True
except ImportError as e:
    CODE_RELATIONSHIP_DETECTOR_AVAILABLE = False
    CodeRelationshipDetector = None  # type: ignore
    import traceback

    print(f"WARNING: CodeRelationshipDetector import failed: {e}")
    traceback.print_exc()
from fastapi import BackgroundTasks, FastAPI, HTTPException, Request
from fastapi.middleware.cors import CORSMiddleware

# Phase 5D: Document Freshness System imports
from freshness import DataRefreshWorker, DocumentFreshnessMonitor, FreshnessDatabase
from freshness.models import (
    FreshnessAnalysis,
    FreshnessAnalysisRequest,
    FreshnessLevel,
    FreshnessStats,
    RefreshPriority,
    RefreshRequest,
    RefreshResult,
)
from models.entity_models import (
    CodeRequest,
    DocumentRequest,
    EntityExtractionResult,
    HealthStatus,
    KnowledgeEntity,
    KnowledgeRelationship,
    RelationshipType,
)
from optimization.performance_optimizer import (
    OptimizationCategory,
    PerformanceOptimizer,
)

# Prometheus metrics
from prometheus_client import CONTENT_TYPE_LATEST, generate_latest
from pydantic import BaseModel, Field
from scoring.quality_scorer import QualityScorer

# Track 3 Autonomous Execution APIs
from src.api.autonomous.routes import router as autonomous_router

# Bridge Intelligence Generation APIs
from src.api.bridge.routes import cleanup_generator
from src.api.bridge.routes import router as bridge_router

# Code Intelligence APIs
from src.api.code_intelligence.routes import router as code_intelligence_router

# Phase 5B: Custom Quality Rules APIs
from src.api.custom_rules.routes import router as custom_rules_router

# Data Quality Monitoring APIs
from src.api.data_quality.routes import router as data_quality_router

# Developer Metrics APIs
from src.api.developer_metrics.routes import router as developer_metrics_router

# POC: File Location APIs
from src.api.file_location.routes import router as file_location_router

# Intelligence Events APIs
from src.api.intelligence_events.routes import router as intelligence_events_router

# Intelligence Metrics APIs
from src.api.intelligence_metrics.routes import router as intelligence_metrics_router

# Knowledge Graph APIs
from src.api.knowledge_graph.routes import router as knowledge_graph_router

# Phase 5A: Pattern Analytics APIs
from src.api.pattern_analytics.routes import router as pattern_analytics_router

# Phase 2 Pattern Learning APIs
from src.api.pattern_learning.routes import router as pattern_learning_router

# Phase 5C: Performance Analytics APIs
from src.api.performance_analytics.routes import router as performance_analytics_router

# Phase 4 Pattern Traceability APIs
from src.api.phase4_traceability.routes import router as phase4_traceability_router

# Platform Health APIs
from src.api.platform.routes import router as platform_health_router

# Phase 5B: Quality Trends APIs
from src.api.quality_trends.routes import router as quality_trends_router

# Event-driven freshness system
from src.events.freshness_event_coordinator import FreshnessEventCoordinator
from src.events.models.document_update_event import (
    DocumentUpdateEvent,
    DocumentUpdateType,
)
from starlette.responses import Response
from storage.memgraph_adapter import MemgraphKnowledgeAdapter

# from python.lib.config_validator import validate_required_env_vars  # TEMP DISABLED: python module not in Docker image

# Pattern Usage Tracking APIs
# TEMPORARILY DISABLED: Missing src.config.database module
# from src.api.pattern_usage.routes import router as pattern_usage_router


# CLAUDE.md generation system (temporarily disabled - module not yet implemented)
# from claude_md import ClaudeMdService
# from claude_md.api_endpoints import claude_md_router


# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Application Constants
# ====================

# HTTP Client Configuration
HTTP_CLIENT_TIMEOUT_SECONDS = 30.0  # Default timeout for HTTP client connections
HTTP_CLIENT_FALLBACK_TIMEOUT_SECONDS = 30.0  # Fallback timeout for one-off clients

# Background Task Retry Configuration
RETRY_MAX_ATTEMPTS = 3  # Maximum number of retry attempts for failed background tasks
RETRY_INITIAL_DELAY_SECONDS = 1.0  # Initial delay before first retry
RETRY_INITIAL_DELAY_FAST_SECONDS = 0.5  # Initial delay for lightweight operations
RETRY_INITIAL_DELAY_SLOW_SECONDS = (
    2.0  # Initial delay for complex multi-step operations
)
RETRY_BACKOFF_MULTIPLIER = 2.0  # Exponential backoff multiplier (e.g., 1s â†’ 2s â†’ 4s)

# Event Priority Levels
EVENT_PRIORITY_HIGH = 8  # High priority for critical operations (new documents)
EVENT_PRIORITY_BACKGROUND = 7  # Lower priority for background processing
EVENT_PRIORITY_NORMAL = 5  # Normal priority for standalone analysis

# Batch Processing Configuration
FRESHNESS_BATCH_TIMEOUT_SECONDS = 30  # Process batches every 30 seconds
FRESHNESS_BATCH_MAX_SIZE = 50  # Or when 50 updates accumulate
BATCH_INDEX_DEFAULT_SIZE = 10  # Default batch size for document indexing
BATCH_PROCESSING_DELAY_SECONDS = (
    0.1  # Small delay between batches to avoid overwhelming services
)

# Performance Baseline Configuration
PERFORMANCE_BASELINE_DURATION_MINUTES = (
    2  # Duration for establishing performance baseline
)
PERFORMANCE_MAX_MEASUREMENTS_DEFAULT = (
    1000  # Default max measurements to keep in memory
)

# Query Limits
QUERY_LIMIT_SMALL = 10  # Default limit for small queries (entities, analyses)
QUERY_LIMIT_MEDIUM = 20  # Default limit for medium queries (relationships)
QUERY_LIMIT_LARGE = 50  # Default limit for large queries (stale documents)

# Confidence Thresholds
CONFIDENCE_THRESHOLD_MIN = 0.0  # Minimum confidence threshold (no filtering)
CONFIDENCE_THRESHOLD_QUALITY = 0.7  # Minimum confidence for quality gates
CONFIDENCE_THRESHOLD_ANOMALY_TOLERANCE_MS = (
    0.1  # Tolerance for zero std_dev anomaly detection
)

# Data Cleanup Configuration
DATA_RETENTION_DAYS_DEFAULT = 90  # Default days to keep old data
DATA_RETENTION_DAYS_MINIMUM = 7  # Minimum days to keep data

# Document Processing Configuration
DOCUMENT_ID_HASH_LENGTH = 12  # Length of hash substring for document IDs
DOCUMENT_CONTENT_PREVIEW_LENGTH = 100  # Length of content preview for hash generation
DOCUMENT_QUALITY_WORD_COUNT_THRESHOLD = 1000  # Word count threshold for quality scoring

# Import comprehensive intelligence logging
from intel_logging.intelligence_logger import (
    intelligence_logger,
)

# Background task utilities with retry logic and metrics
from src.utils.background_task_utils import retry_background_task
from src.utils.background_task_status_tracker import (
    BackgroundTaskStatusTracker,
    get_global_tracker,
    set_global_tracker,
)
from src.utils.background_task_tracker_init import initialize_background_task_tracker

# TODO: Re-enable pipeline correlation after fixing shared module access
# from shared.logging.pipeline_correlation import CorrelationHeaders, get_pipeline_correlation

# Global service components
intelligence_service = None
memgraph_adapter = None
performance_optimizer = None
code_relationship_detector = None
background_task_tracker = None  # Background task status tracking

# Phase 5C: Performance Baseline Service
performance_baseline_service = None

# Phase 5D: Document Freshness System components
freshness_monitor = None
freshness_database = None
freshness_worker = None
freshness_event_coordinator = None

# CLAUDE.md generation service
claude_md_service = None

# MVP Day 3: Kafka Consumer for event-driven intelligence handlers
kafka_consumer = None

# Pattern Usage Tracking Consumer
usage_tracking_consumer = None

# Auto-Indexing Service for automatic project indexing
auto_indexer = None

# Shared HTTP client for background tasks (connection pooling)
shared_http_client = None

# Health monitoring service for infrastructure health checks
health_monitor = None
health_check_task = None

# Directory hierarchy indexer for project structure visualization
directory_indexer = None

# Orphan file detector for identifying unreachable/unused files
orphan_detector = None


@asynccontextmanager
async def lifespan(app: FastAPI):
    """Initialize and cleanup service components"""
    global intelligence_service, memgraph_adapter, performance_optimizer
    global performance_baseline_service
    global freshness_monitor, freshness_database, freshness_worker, freshness_event_coordinator
    global claude_md_service, kafka_consumer, usage_tracking_consumer, auto_indexer, shared_http_client
    global health_monitor, health_check_task, directory_indexer, orphan_detector
    global code_relationship_detector, background_task_tracker

    try:
        # Validate environment variables before any initialization
        # validate_required_env_vars()  # TEMP DISABLED: python module not in Docker image

        # Log startup sequence start
        intelligence_logger.log_startup_phase("initialization", "start")

        # Initialize shared HTTP client with connection pooling for background tasks
        from src.config.http_client_config import create_search_service_client

        shared_http_client = create_search_service_client(
            timeout_override=HTTP_CLIENT_TIMEOUT_SECONDS
        )
        intelligence_logger.log_startup_phase("shared_http_client", "success")

        # Initialize background task status tracker
        intelligence_logger.log_startup_phase("background_task_tracker", "progress")
        background_task_tracker = await initialize_background_task_tracker()
        intelligence_logger.log_startup_phase("background_task_tracker", "success")

        # Initialize Memgraph adapter
        intelligence_logger.log_startup_phase("memgraph_adapter", "progress")
        memgraph_uri = os.getenv("MEMGRAPH_URI", "bolt://localhost:7687")
        memgraph_adapter = MemgraphKnowledgeAdapter(uri=memgraph_uri)
        await memgraph_adapter.initialize()
        intelligence_logger.log_startup_phase(
            "memgraph_adapter", "success", {"memgraph_uri": memgraph_uri}
        )

        # Initialize Code Relationship Detector for file import extraction (REQUIRED)
        if CODE_RELATIONSHIP_DETECTOR_AVAILABLE:
            intelligence_logger.log_startup_phase(
                "code_relationship_detector", "progress"
            )
            code_relationship_detector = CodeRelationshipDetector()
            intelligence_logger.log_startup_phase(
                "code_relationship_detector", "success"
            )
            logger.info(
                "âœ… CodeRelationshipDetector initialized successfully - file import extraction ENABLED"
            )
        else:
            logger.error(
                "âŒ CodeRelationshipDetector NOT available - this is REQUIRED for file import extraction"
            )
            logger.error(
                "âŒ File tree/graph implementation will NOT work without this component"
            )
            code_relationship_detector = None

        # Initialize Directory Hierarchy Indexer for project structure visualization
        intelligence_logger.log_startup_phase("directory_indexer", "progress")
        from src.services.directory_indexer import DirectoryIndexer

        directory_indexer = DirectoryIndexer(memgraph_adapter)
        intelligence_logger.log_startup_phase("directory_indexer", "success")

        # Initialize Orphan File Detector for identifying unreachable/unused files
        intelligence_logger.log_startup_phase("orphan_detector", "progress")
        from src.services.orphan_detector import OrphanDetector

        orphan_detector = OrphanDetector(memgraph_adapter)
        intelligence_logger.log_startup_phase("orphan_detector", "success")

        # Initialize performance optimizer
        intelligence_logger.log_startup_phase("performance_optimizer", "progress")
        db_connection_string = os.getenv(
            "DATABASE_URL", "postgresql://localhost/archon"
        )
        performance_optimizer = PerformanceOptimizer(db_connection_string)
        intelligence_logger.log_startup_phase("performance_optimizer", "success")

        # Phase 5C: Initialize Performance Baseline Service
        intelligence_logger.log_startup_phase(
            "performance_baseline_service", "progress"
        )
        from src.archon_services.performance.baseline_service import (
            PerformanceBaselineService,
        )

        # Configure max_measurements via environment variable for memory-constrained deployments
        # Default: 1000 measurements (~1-2MB memory per operation)
        max_measurements = int(
            os.getenv(
                "PERFORMANCE_MAX_MEASUREMENTS",
                str(PERFORMANCE_MAX_MEASUREMENTS_DEFAULT),
            )
        )
        performance_baseline_service = PerformanceBaselineService(
            max_measurements=max_measurements
        )
        intelligence_logger.log_startup_phase(
            "performance_baseline_service",
            "success",
            {"max_measurements": max_measurements},
        )

        # Phase 5D: Initialize Document Freshness System with direct PostgreSQL access
        # PostgreSQL connection configured via environment variables or defaults
        # Made optional for testing environments where PostgreSQL may not be available
        try:
            freshness_database = FreshnessDatabase()
            await freshness_database.initialize()

            freshness_monitor = DocumentFreshnessMonitor()
            freshness_worker = DataRefreshWorker(freshness_database, freshness_monitor)

            # Initialize event-driven freshness coordinator
            freshness_event_coordinator = FreshnessEventCoordinator(
                freshness_monitor=freshness_monitor,
                freshness_database=freshness_database,
                batch_timeout_seconds=FRESHNESS_BATCH_TIMEOUT_SECONDS,
                max_batch_size=FRESHNESS_BATCH_MAX_SIZE,
            )
            intelligence_logger.log_startup_phase("freshness_system", "success")
        except Exception as e:
            logger.warning(
                f"Failed to initialize Freshness System (PostgreSQL): {e}. "
                "Freshness features will be disabled. This is expected in test environments."
            )
            freshness_database = None
            freshness_monitor = None
            freshness_worker = None
            freshness_event_coordinator = None
            intelligence_logger.log_startup_phase(
                "freshness_system", "skipped", {"reason": "PostgreSQL not available"}
            )

        # Establish baseline for key operations (run in background to not block startup)
        asyncio.create_task(
            performance_optimizer.establish_performance_baseline(
                "entity_extraction",
                duration_minutes=PERFORMANCE_BASELINE_DURATION_MINUTES,
            )
        )

        # Initialize intelligence service with enhanced extraction
        intelligence_service = EnhancedEntityExtractor(
            memgraph_adapter=memgraph_adapter,
            embedding_model_url=os.getenv(
                "EMBEDDING_MODEL_URL", "http://192.168.86.201:8002"
            ),
        )

        # Initialize CLAUDE.md generation service (temporarily disabled)
        # claude_md_service = ClaudeMdService(
        #     intelligence_service=intelligence_service,
        #     ollama_url=os.getenv("OLLAMA_BASE_URL", "http://192.168.86.200:11434")
        # )

        # MVP Day 3: Initialize and start Kafka consumer for codegen events
        kafka_enabled = os.getenv("KAFKA_ENABLE_CONSUMER", "true").lower() == "true"

        if kafka_enabled:
            try:
                intelligence_logger.log_startup_phase("kafka_consumer", "progress")
                from src.kafka_consumer import get_kafka_consumer

                kafka_consumer = get_kafka_consumer()
                await kafka_consumer.initialize()

                # Start Kafka consumer as background task (consumer loop is infinite)
                kafka_consumer_task = asyncio.create_task(kafka_consumer.start())

                # Add error handler callback to monitor task health
                def handle_consumer_error(task: asyncio.Task):
                    if task.cancelled():
                        logger.info("Kafka consumer task cancelled")
                        return
                    exc = task.exception()
                    if exc:
                        logger.error(
                            f"Kafka consumer task failed: {exc}",
                            exc_info=(type(exc), exc, exc.__traceback__),
                        )

                kafka_consumer_task.add_done_callback(handle_consumer_error)

                intelligence_logger.log_startup_phase(
                    "kafka_consumer",
                    "success",
                    {
                        "bootstrap_servers": kafka_consumer.bootstrap_servers,
                        "topics": kafka_consumer.topics or [],
                        "handlers_registered": len(kafka_consumer.handlers),
                    },
                )
                logger.info(
                    f"Kafka consumer started successfully | topics={kafka_consumer.topics or []} | "
                    f"handlers={len(kafka_consumer.handlers)}"
                )
            except Exception as kafka_error:
                logger.error(
                    f"Failed to start Kafka consumer: {kafka_error}",
                    exc_info=True,
                )
                logger.warning(
                    "Intelligence service will continue without Kafka consumer. "
                    "Event-driven handlers will not process events."
                )
                kafka_consumer = None
        else:
            logger.info("Kafka consumer disabled via KAFKA_ENABLE_CONSUMER=false")

        # Pattern Usage Tracking Consumer: Track pattern usage from Kafka events
        if kafka_enabled:
            try:
                intelligence_logger.log_startup_phase(
                    "usage_tracking_consumer", "progress"
                )
                from src.config.database import get_db_pool
                from src.usage_tracking.kafka_consumer import UsageTrackingConsumer

                db_pool = await get_db_pool()
                usage_tracking_consumer = UsageTrackingConsumer(db_pool)
                await usage_tracking_consumer.start()

                intelligence_logger.log_startup_phase(
                    "usage_tracking_consumer",
                    "success",
                    {
                        "topics": usage_tracking_consumer.topics,
                        "group_id": usage_tracking_consumer.group_id,
                    },
                )
                logger.info(
                    f"Usage tracking consumer started | topics={usage_tracking_consumer.topics}"
                )
            except Exception as usage_error:
                logger.error(
                    f"Failed to start usage tracking consumer: {usage_error}",
                    exc_info=True,
                )
                logger.warning(
                    "Intelligence service will continue without usage tracking consumer. "
                    "Pattern usage will not be tracked automatically."
                )
                usage_tracking_consumer = None
        else:
            logger.info("Usage tracking consumer disabled (Kafka disabled)")

        # Auto-Indexing Service: Initialize and start automatic project indexing
        try:
            intelligence_logger.log_startup_phase("auto_indexer", "progress")
            from src.archon_services.auto_indexer import AutoIndexerService
            from src.events.kafka_publisher import KafkaEventPublisher

            # Create dedicated event publisher for AutoIndexerService
            # (separate from consumer's internal DLQ publisher)
            event_router = None
            if kafka_consumer:
                event_router = KafkaEventPublisher()
                await event_router.initialize()
                logger.info(
                    "Created dedicated KafkaEventPublisher for AutoIndexerService"
                )

            auto_indexer = AutoIndexerService(event_router=event_router)
            await auto_indexer.start()

            intelligence_logger.log_startup_phase(
                "auto_indexer",
                "success",
                {
                    "enabled": auto_indexer.enabled,
                    "projects_configured": len(auto_indexer.projects),
                    "index_on_startup": auto_indexer.index_on_startup,
                    "watch_changes": auto_indexer.watch_changes,
                    "schedule_hours": auto_indexer.schedule_hours,
                },
            )
            logger.info(
                f"Auto-Indexing Service started successfully | "
                f"projects={len(auto_indexer.projects)} | "
                f"startup={auto_indexer.index_on_startup} | "
                f"schedule={auto_indexer.schedule_hours}h"
            )
        except Exception as auto_indexer_error:
            logger.error(
                f"Failed to start Auto-Indexing Service: {auto_indexer_error}",
                exc_info=True,
            )
            logger.warning(
                "Intelligence service will continue without Auto-Indexing Service. "
                "Projects will not be automatically indexed."
            )
            auto_indexer = None

        # Phase 5C: Initialize Performance Analytics API with baseline service
        from src.api.performance_analytics.routes import (
            initialize_services as init_perf_analytics,
        )

        init_perf_analytics(performance_baseline_service)
        logger.info("Performance Analytics API initialized with baseline service")

        # Initialize Intelligence Metrics API with database pool
        try:
            intelligence_logger.log_startup_phase(
                "intelligence_metrics_api", "progress"
            )
            import asyncpg
            from src.api.intelligence_metrics.routes import initialize_db_pool

            # Get database URL from environment (same as traceability DB)
            intelligence_db_url = (
                os.getenv("TRACEABILITY_DB_URL_EXTERNAL")
                or os.getenv("TRACEABILITY_DB_URL")
                or os.getenv("DATABASE_URL", "postgresql://localhost/archon")
            )

            # Create connection pool
            intelligence_db_pool = await asyncpg.create_pool(
                intelligence_db_url,
                min_size=2,
                max_size=10,
                command_timeout=10.0,
                server_settings={
                    "application_name": "intelligence_metrics_api",
                    "jit": "off",
                },
            )

            # Initialize the API with the pool
            initialize_db_pool(intelligence_db_pool)

            intelligence_logger.log_startup_phase(
                "intelligence_metrics_api",
                "success",
                {
                    "database_url": intelligence_db_url.split("@")[-1]
                },  # Log without credentials
            )
            logger.info("Intelligence Metrics API initialized with database pool")
        except Exception as intel_metrics_error:
            logger.error(
                f"Failed to initialize Intelligence Metrics API: {intel_metrics_error}",
                exc_info=True,
            )
            logger.warning(
                "Intelligence service will continue without Intelligence Metrics API. "
                "Quality impact tracking will not be available."
            )

        logger.info(
            "Intelligence service with document freshness and CLAUDE.md generation initialized successfully"
        )

        # Initialize Health Monitoring Service with periodic background checks
        try:
            intelligence_logger.log_startup_phase("health_monitor", "progress")
            from src.archon_services.health_monitor import HealthMonitor

            health_monitor = HealthMonitor.from_env()

            # Background task for periodic health checks
            async def periodic_health_check():
                """Run health checks every 30 seconds in background"""
                await asyncio.sleep(10)  # Initial delay to let services stabilize
                while True:
                    try:
                        await health_monitor.check_all_services(use_cache=False)
                        logger.debug("Periodic health check completed successfully")
                    except Exception as e:
                        logger.error(
                            f"Periodic health check failed: {e}", exc_info=True
                        )
                    await asyncio.sleep(30)  # Check every 30 seconds

            # Start background task
            health_check_task = asyncio.create_task(periodic_health_check())

            # Add error handler callback
            def handle_health_check_error(task: asyncio.Task):
                if task.cancelled():
                    logger.info("Health check task cancelled")
                    return
                exc = task.exception()
                if exc:
                    logger.error(
                        f"Health check task failed: {exc}",
                        exc_info=(type(exc), exc, exc.__traceback__),
                    )

            health_check_task.add_done_callback(handle_health_check_error)

            intelligence_logger.log_startup_phase(
                "health_monitor",
                "success",
                {
                    "qdrant_host": health_monitor.qdrant_host,
                    "postgres_host": health_monitor.postgres_host,
                    "kafka_servers": health_monitor.kafka_bootstrap_servers,
                    "cache_ttl": health_monitor.cache_ttl,
                },
            )
            logger.info(
                f"Health Monitor started successfully | "
                f"qdrant={health_monitor.qdrant_host}:{health_monitor.qdrant_port} | "
                f"postgres={health_monitor.postgres_host}:{health_monitor.postgres_port} | "
                f"kafka={health_monitor.kafka_bootstrap_servers}"
            )
        except Exception as health_error:
            logger.error(
                f"Failed to start Health Monitor: {health_error}",
                exc_info=True,
            )
            logger.warning(
                "Intelligence service will continue without Health Monitor. "
                "Infrastructure health checks will not be cached."
            )
            health_monitor = None

        # Log all registered routes for debugging
        logger.info("ðŸ“‹ [STARTUP] Registered routes:")
        route_count = 0
        for route in app.routes:
            if hasattr(route, "path") and hasattr(route, "methods"):
                methods = ", ".join(route.methods) if route.methods else "N/A"
                logger.info(f"  - {methods:8} {route.path}")
                route_count += 1
            elif hasattr(route, "path"):
                logger.info(f"  - {'MOUNT':8} {route.path}")
                route_count += 1
        logger.info(f"ðŸ“‹ [STARTUP] Total routes registered: {route_count}")

        yield

    except Exception as e:
        logger.error(f"Failed to initialize intelligence service: {e}")
        raise
    finally:
        # Cleanup
        if health_check_task:
            logger.info("Stopping Health Monitor background task...")
            health_check_task.cancel()
            try:
                await health_check_task
            except asyncio.CancelledError:
                pass
        if health_monitor:
            logger.info("Cleaning up Health Monitor...")
            await health_monitor.cleanup()
        if auto_indexer:
            logger.info("Stopping Auto-Indexing Service...")
            await auto_indexer.shutdown()
        if kafka_consumer:
            logger.info("Stopping Kafka consumer...")
            await kafka_consumer.stop()
        if usage_tracking_consumer:
            logger.info("Stopping usage tracking consumer...")
            await usage_tracking_consumer.stop()
        if memgraph_adapter:
            await memgraph_adapter.close()
        if freshness_event_coordinator:
            await freshness_event_coordinator.shutdown()
        if freshness_database:
            await freshness_database.close()
        if shared_http_client:
            logger.info("Closing shared HTTP client...")
            await shared_http_client.aclose()
        # Cleanup Bridge Intelligence Generator (LangExtract client)
        logger.info("Shutting down Bridge Intelligence Generator...")
        await cleanup_generator()
        logger.info("Intelligence service shutdown complete")


# FastAPI application
app = FastAPI(
    title="Archon Intelligence Service",
    description="Entity extraction and semantic analysis for knowledge graph population",
    version="1.0.0",
    lifespan=lifespan,
)


# CORS Configuration (environment-based for production security)
def get_cors_origins() -> list[str]:
    """
    Get CORS allowed origins from environment with production validation.

    Returns:
        List of allowed origins. Defaults to localhost for development.
    """
    # Check both CORS_ALLOWED_ORIGINS and ALLOWED_ORIGINS (for backward compatibility)
    origins_env = os.getenv("CORS_ALLOWED_ORIGINS") or os.getenv("ALLOWED_ORIGINS")
    environment = os.getenv("ENVIRONMENT", "development")

    # Development defaults (localhost variants)
    dev_origins = [
        "http://localhost:3737",
        "http://localhost:8181",
        "http://127.0.0.1:3737",
        "http://127.0.0.1:8181",
    ]

    if not origins_env:
        if environment == "production":
            logger.warning(
                "âš ï¸  SECURITY WARNING: CORS_ALLOWED_ORIGINS not set in production! "
                "Using restrictive defaults. Set CORS_ALLOWED_ORIGINS environment variable."
            )
            return dev_origins  # Fail-safe: use localhost only
        else:
            logger.info("CORS: Using development defaults (localhost)")
            return dev_origins

    # Wildcard check for production
    if origins_env == "*":
        if environment == "production":
            logger.error(
                "âŒ SECURITY ERROR: Wildcard CORS (*) is NOT allowed in production! "
                "Set CORS_ALLOWED_ORIGINS to specific domains. Using restrictive defaults."
            )
            return dev_origins  # Fail-safe: reject wildcard in production
        else:
            logger.warning("CORS: Using wildcard (*) - development only")
            return ["*"]

    # Parse comma-separated origins
    origins = [origin.strip() for origin in origins_env.split(",")]
    logger.info(f"CORS: Configured {len(origins)} allowed origins for {environment}")

    return origins


cors_origins = get_cors_origins()

# CORS middleware with environment-based configuration
app.add_middleware(
    CORSMiddleware,
    allow_origins=cors_origins,
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)


# Request logging middleware for debugging routing issues
@app.middleware("http")
async def log_all_requests(request: Request, call_next):
    """Log all incoming requests to help debug routing issues"""
    start_time = time.time()
    logger.info(
        f"ðŸ” [REQUEST] {request.method} {request.url.path} | "
        f"query={request.url.query} | client={request.client.host if request.client else 'unknown'}"
    )

    try:
        response = await call_next(request)
        duration_ms = (time.time() - start_time) * 1000
        logger.info(
            f"âœ… [RESPONSE] {request.method} {request.url.path} | "
            f"status={response.status_code} | duration={duration_ms:.2f}ms"
        )
        return response
    except Exception as e:
        duration_ms = (time.time() - start_time) * 1000
        logger.error(
            f"âŒ [ERROR] {request.method} {request.url.path} | "
            f"error={str(e)} | duration={duration_ms:.2f}ms"
        )
        raise


# Include CLAUDE.md generation routes
# app.include_router(claude_md_router)  # Temporarily disabled - module not yet implemented

# Include Track 3 Autonomous Execution API routes
app.include_router(autonomous_router)

# Include Phase 2 Pattern Learning API routes
app.include_router(pattern_learning_router)

# Include Phase 4 Pattern Traceability API routes
app.include_router(phase4_traceability_router)

# Include Bridge Intelligence Generation API routes
app.include_router(bridge_router)

# Include Phase 5B Quality Trends API routes
app.include_router(quality_trends_router)

# Include Phase 5A Pattern Analytics API routes
app.include_router(pattern_analytics_router)

# Include Pattern Usage Tracking API routes
# TEMPORARILY DISABLED: Missing src.config.database module
# app.include_router(pattern_usage_router)

# Include Phase 5C Performance Analytics API routes
app.include_router(performance_analytics_router)

# Include Phase 5B Custom Quality Rules API routes
app.include_router(custom_rules_router)

# Include Data Quality Monitoring API routes
app.include_router(data_quality_router)

# Include Intelligence Metrics API routes
app.include_router(intelligence_metrics_router)

# Include Intelligence Events API routes
app.include_router(intelligence_events_router)

# Include Developer Metrics API routes
app.include_router(developer_metrics_router)

# Include Knowledge Graph API routes
app.include_router(knowledge_graph_router)

# Include POC File Location API routes
app.include_router(file_location_router)

# Include Platform Health API routes
app.include_router(platform_health_router)

# Include Code Intelligence API routes
app.include_router(code_intelligence_router)


# Helper function for vLLM health check
async def _check_vllm_health() -> bool:
    """
    Check vLLM embedding service health.

    Returns True if service is accessible, False otherwise.
    """
    vllm_url = os.getenv("EMBEDDING_MODEL_URL", "http://192.168.86.201:8002")

    try:
        async with httpx.AsyncClient(timeout=1.0) as client:
            # Try to access the models endpoint (OpenAI compatible)
            response = await client.get(f"{vllm_url}/v1/models")
            return response.status_code == 200
    except Exception as e:
        logger.debug(f"vLLM health check failed: {e}")
        return False


@app.get("/health", response_model=HealthStatus)
async def health_check(request: Request):
    """
    Service health check endpoint with comprehensive logging and timeouts.

    Returns "healthy" if core services (Memgraph) are operational.
    Freshness database is optional and won't cause "degraded" status.
    """
    time.time()

    # Extract correlation headers and set context
    # TODO: Re-enable pipeline correlation after fixing shared module access
    # correlation_headers = CorrelationHeaders.extract_headers(dict(request.headers))
    # For now, use empty correlation headers
    correlation_headers = {}
    intelligence_logger.set_request_context(
        request_id=correlation_headers.get("request_id"),
        correlation_id=correlation_headers.get("correlation_id"),
        pipeline_correlation_id=correlation_headers.get("pipeline_id"),
    )

    try:
        # Test Memgraph connectivity (core service - required for "healthy" status)
        # Wrap in timeout to prevent blocking health checks
        memgraph_status = False
        if memgraph_adapter:
            try:
                memgraph_status = await asyncio.wait_for(
                    memgraph_adapter.health_check(),
                    timeout=0.1,  # 100ms timeout for Memgraph check (fast query)
                )
            except asyncio.TimeoutError:
                logger.warning("Memgraph health check timed out (100ms)")
                memgraph_status = False
            except Exception as e:
                logger.warning(f"Memgraph health check failed: {e}")
                memgraph_status = False

        # Test vLLM embedding service connectivity (optional - doesn't affect health status)
        embedding_status = await _check_vllm_health()

        # Test freshness database connectivity with timeout (optional service)
        # Freshness DB is optional and doesn't affect primary health status
        freshness_db_status = False
        if freshness_database:
            try:
                # Check freshness DB with 500ms timeout to keep health check fast
                freshness_db_status = await asyncio.wait_for(
                    freshness_database.health_check(timeout_seconds=0.5), timeout=0.5
                )
            except (asyncio.TimeoutError, Exception) as e:
                logger.debug(f"Freshness DB health check failed: {e}")
                freshness_db_status = False

        # Service is "healthy" if core service (Memgraph) is operational
        # Freshness database and embedding service are optional and don't affect primary health status
        status = "healthy" if memgraph_status else "degraded"

        health_status = {
            "status": status,
            "memgraph_connected": memgraph_status,
            "embedding_service_connected": embedding_status,
            "freshness_database_connected": freshness_db_status,
            "service_version": "1.0.0",
        }

        # Log health check results
        intelligence_logger.log_health_check(health_status)

        return HealthStatus(**health_status)

    except Exception as e:
        logger.error(f"Health check failed: {e}")
        health_status = {
            "status": "unhealthy",
            "memgraph_connected": False,
            "embedding_service_connected": False,
            "service_version": "1.0.0",
            "error": str(e),
        }
        intelligence_logger.log_health_check(health_status)
        return HealthStatus(**health_status)


@app.get("/cache/health")
async def cache_health_check():
    """
    Valkey cache health check endpoint with connection monitoring and metrics.

    Returns comprehensive cache health status including:
    - Connection status and latency
    - Memory usage
    - Hit rate statistics
    - Server information
    - Cache configuration

    Response Codes:
    - 200: Cache healthy or disabled
    - 503: Cache unavailable or unhealthy
    """
    import redis.asyncio as aioredis

    # Check if cache is enabled
    cache_enabled = os.getenv("ENABLE_CACHE", "true").lower() == "true"

    if not cache_enabled:
        return {
            "status": "disabled",
            "cache_type": "valkey",
            "connected": False,
            "message": "Cache is disabled via ENABLE_CACHE environment variable",
            "timestamp": datetime.now(timezone.utc).isoformat(),
        }

    # Try to connect to Valkey
    valkey_url = os.getenv("VALKEY_URL", "redis://archon-valkey:6379/0")
    redis_client = None

    try:
        # Create Redis client with timeout
        redis_client = await aioredis.from_url(
            valkey_url,
            decode_responses=True,
            socket_connect_timeout=2,  # 2 second timeout
        )

        # Measure ping latency
        start_time = time.time()
        await redis_client.ping()
        latency_ms = (time.time() - start_time) * 1000

        # Get server info
        server_info = await redis_client.info("server")
        memory_info = await redis_client.info("memory")
        stats_info = await redis_client.info("stats")

        # Calculate hit rate
        hits = stats_info.get("keyspace_hits", 0)
        misses = stats_info.get("keyspace_misses", 0)
        total_ops = hits + misses
        hit_rate = (hits / total_ops) if total_ops > 0 else 0.0

        # Get memory usage
        memory_used_bytes = memory_info.get("used_memory", 0)
        memory_used_mb = memory_used_bytes / (1024 * 1024)
        memory_limit_mb = (
            memory_info.get("maxmemory", 0) / (1024 * 1024)
            if memory_info.get("maxmemory", 0) > 0
            else 512
        )  # Default 512MB

        # Determine health status based on latency
        if latency_ms < 10:
            status = "healthy"
        elif latency_ms < 100:
            status = "degraded"
        else:
            status = "slow"

        response = {
            "status": status,
            "cache_type": "valkey",
            "connection": "connected",
            "connected": True,
            "latency_ms": round(latency_ms, 2),
            "memory_used_mb": round(memory_used_mb, 2),
            "memory_limit_mb": int(memory_limit_mb),
            "memory_used_human": memory_info.get("used_memory_human", "0B"),
            "hit_rate": round(hit_rate, 4),
            "total_hits": hits,
            "total_misses": misses,
            "uptime_seconds": server_info.get("uptime_in_seconds", 0),
            "valkey_version": server_info.get("redis_version", "unknown"),
            "valkey_mode": server_info.get("redis_mode", "standalone"),
            "timestamp": datetime.now(timezone.utc).isoformat(),
        }

        # Log health check
        logger.info(
            f"Cache health check: {status} (latency: {latency_ms:.2f}ms, hit_rate: {hit_rate:.2%})"
        )

        return response

    except asyncio.TimeoutError:
        logger.warning(f"Cache health check timeout connecting to {valkey_url}")
        return {
            "status": "unavailable",
            "cache_type": "valkey",
            "connection": "timeout",
            "connected": False,
            "error": "Connection timeout after 2 seconds",
            "message": f"Failed to connect to cache at {valkey_url}",
            "timestamp": datetime.now(timezone.utc).isoformat(),
        }

    except Exception as e:
        logger.error(f"Cache health check failed: {e}")
        return {
            "status": "unavailable",
            "cache_type": "valkey",
            "connection": "failed",
            "connected": False,
            "error": str(e),
            "message": f"Failed to connect to cache: {str(e)}",
            "timestamp": datetime.now(timezone.utc).isoformat(),
        }

    finally:
        # Clean up Redis connection
        if redis_client:
            try:
                await redis_client.close()
            except Exception:
                pass  # Ignore cleanup errors


@app.post("/extract/document", response_model=EntityExtractionResult)
async def extract_document_entities(
    request: DocumentRequest, background_tasks: BackgroundTasks, http_request: Request
) -> EntityExtractionResult:
    """Extract entities from document content with comprehensive logging"""
    extraction_start_time = time.time()

    # Extract correlation headers and set context
    # TODO: Re-enable pipeline correlation after fixing shared module access
    # correlation_headers = CorrelationHeaders.extract_headers(dict(http_request.headers))
    # For now, use empty correlation headers
    correlation_headers = {}
    intelligence_logger.set_request_context(
        request_id=correlation_headers.get("request_id"),
        correlation_id=correlation_headers.get("correlation_id"),
        pipeline_correlation_id=correlation_headers.get("pipeline_id"),
    )

    # Generate document ID for tracking using helper function
    document_id = _generate_document_id(source_path=request.source_path)
    project_id = "unknown_project"  # TODO: Extract from metadata

    # Log document processing start
    intelligence_logger.log_document_processing_start(
        document_id=document_id,
        project_id=project_id,
        content_length=len(request.content),
        processing_details={
            "source_path": request.source_path,
            "metadata": request.metadata or {},
        },
    )

    try:
        if not intelligence_service:
            intelligence_logger.log_document_processing_error(
                document_id=document_id,
                error=Exception("Intelligence service not initialized"),
                processing_stage="initialization_check",
            )
            raise HTTPException(
                status_code=503, detail="Intelligence service not initialized"
            )

        # Entity extraction will be logged after completion with results

        # Extract entities and relationships from document content
        entities, relationships_dict = (
            await intelligence_service.extract_entities_from_document(
                content=request.content,
                source_path=request.source_path,
                metadata=request.metadata or {},
            )
        )

        # Convert relationships to model objects
        relationships = _convert_relationships_to_models(relationships_dict)

        # Log extraction results
        logger.info(
            f"Extracted {len(entities)} entities and {len(relationships)} relationships from document"
        )

        # Store entities and relationships in background if requested
        if request.store_entities and entities:
            background_tasks.add_task(
                _process_document_background,
                entities,
                request.source_path,
                request.content,  # Pass full content for vectorization
                request.metadata or {},
                _generate_document_id(
                    source_path=request.source_path
                ),  # Generate document_id using helper
                "unknown_project",  # TODO: Extract project_id from source_path
                relationships,  # Pass relationships to background task
            )
        # If not storing entities but freshness analysis requested, trigger immediately
        elif request.trigger_freshness_analysis and freshness_event_coordinator:
            background_tasks.add_task(
                _trigger_freshness_analysis_background, request.source_path
            )

        # Calculate processing time
        processing_time_ms = (time.time() - extraction_start_time) * 1000

        # Calculate confidence statistics
        confidence_stats = {
            "mean": (
                sum(e.confidence_score for e in entities) / len(entities)
                if entities
                else 0
            ),
            "min": min(e.confidence_score for e in entities) if entities else 0,
            "max": max(e.confidence_score for e in entities) if entities else 0,
        }

        # Log entity extraction results
        entity_types = (
            list(set(e.entity_type.value for e in entities)) if entities else []
        )
        intelligence_logger.log_entity_extraction(
            document_id=document_id,
            source_path=request.source_path,
            entities_found=len(entities),
            extraction_time_ms=processing_time_ms,
            entity_types=entity_types,
        )

        # Log successful document processing completion
        vectorization_result = {
            "entities_count": len(entities),
            "processing_time_ms": processing_time_ms,
            "confidence_stats": confidence_stats,
            "background_processing": request.store_entities,
        }
        intelligence_logger.log_document_processing_complete(
            document_id=document_id,
            entities_extracted=len(entities),
            vectorization_result=vectorization_result,
        )

        return EntityExtractionResult(
            entities=entities,
            total_count=len(entities),
            processing_time_ms=processing_time_ms,
            confidence_stats=confidence_stats,
        )

    except HTTPException:
        raise
    except Exception as e:
        # Log document processing error
        processing_time_ms = (time.time() - extraction_start_time) * 1000
        intelligence_logger.log_document_processing_error(
            document_id=document_id, error=e, processing_stage="entity_extraction"
        )
        logger.error(f"Document entity extraction failed: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@app.post("/extract/code", response_model=EntityExtractionResult)
async def extract_code_entities(
    request: CodeRequest, background_tasks: BackgroundTasks
) -> EntityExtractionResult:
    """Extract entities from code content"""
    try:
        if not intelligence_service:
            raise HTTPException(
                status_code=503, detail="Intelligence service not initialized"
            )

        # Extract entities from code content
        entities = await intelligence_service.extract_entities_from_code(
            content=request.content,
            source_path=request.source_path,
            language=request.language,
            metadata=request.metadata or {},
        )

        # Store entities in background if requested
        if request.store_entities and entities:
            background_tasks.add_task(
                _store_entities_background,
                entities,
                request.source_path,
                request.trigger_freshness_analysis,
            )
        # If not storing entities but freshness analysis requested, trigger immediately
        elif request.trigger_freshness_analysis and freshness_event_coordinator:
            background_tasks.add_task(
                _trigger_freshness_analysis_background, request.source_path
            )

        return EntityExtractionResult(
            entities=entities,
            total_count=len(entities),
            processing_time_ms=0,  # TODO: Add timing
            confidence_stats={
                "mean": (
                    sum(e.confidence_score for e in entities) / len(entities)
                    if entities
                    else 0
                ),
                "min": min(e.confidence_score for e in entities) if entities else 0,
                "max": max(e.confidence_score for e in entities) if entities else 0,
            },
        )

    except Exception as e:
        logger.error(f"Code entity extraction failed: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@app.get("/entities/search")
async def search_entities(
    query: str,
    entity_type: Optional[str] = None,
    limit: int = QUERY_LIMIT_SMALL,
    min_confidence: float = CONFIDENCE_THRESHOLD_MIN,
) -> List[KnowledgeEntity]:
    """Search entities by name, type, and properties"""
    try:
        if not memgraph_adapter:
            raise HTTPException(
                status_code=503, detail="Memgraph adapter not initialized"
            )

        entities = await memgraph_adapter.search_entities(
            query=query,
            entity_type=entity_type,
            limit=limit,
            min_confidence=min_confidence,
        )

        return entities

    except Exception as e:
        logger.error(f"Entity search failed: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@app.get("/relationships/{entity_id}")
async def get_entity_relationships(
    entity_id: str,
    relationship_type: Optional[str] = None,
    limit: int = QUERY_LIMIT_MEDIUM,
):
    """Get relationships for a specific entity"""
    try:
        if not memgraph_adapter:
            raise HTTPException(
                status_code=503, detail="Memgraph adapter not initialized"
            )

        relationships = await memgraph_adapter.get_entity_relationships(
            entity_id=entity_id, relationship_type=relationship_type, limit=limit
        )

        return relationships

    except Exception as e:
        logger.error(f"Failed to get relationships for entity {entity_id}: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@retry_background_task(
    max_retries=RETRY_MAX_ATTEMPTS,
    initial_delay=RETRY_INITIAL_DELAY_SECONDS,
    backoff_multiplier=RETRY_BACKOFF_MULTIPLIER,
    operation_name="store_entities",
    operation_type="entity_storage",
)
async def _store_entities_background(
    entities: List[KnowledgeEntity], source_path: str, trigger_freshness: bool = True
):
    """
    Background task to store entities in Memgraph and trigger freshness analysis.

    Includes retry logic with exponential backoff:
    - Max retries: 3
    - Initial delay: 1s
    - Backoff multiplier: 2x (1s â†’ 2s â†’ 4s)
    - Tracks metrics via Prometheus

    Args:
        entities: List of knowledge entities to store
        source_path: Source path of the document
        trigger_freshness: Whether to trigger freshness analysis (default: True)

    Raises:
        Exception: Propagated after all retries exhausted (for metrics tracking)
    """
    # Store entities in Memgraph
    if memgraph_adapter:
        await memgraph_adapter.store_entities(entities)
        logger.info(
            f"âœ… Stored {len(entities)} entities from {source_path}",
            extra={
                "source_path": source_path,
                "entities_count": len(entities),
                "operation": "entity_storage",
            },
        )
    else:
        logger.warning(
            "âš ï¸ Memgraph adapter not available - skipping entity storage",
            extra={"source_path": source_path},
        )

    # Trigger automatic freshness analysis for the updated document
    if trigger_freshness and freshness_event_coordinator:
        event = DocumentUpdateEvent(
            event_type=DocumentUpdateType.UPDATED,
            document_path=source_path,
            priority=EVENT_PRIORITY_BACKGROUND,
            requires_immediate_analysis=False,  # Use batch processing
            created_by="intelligence_service_auto",
        )
        await freshness_event_coordinator.handle_document_update_event(event)
        logger.info(
            f"âœ… Triggered freshness analysis for updated document: {source_path}",
            extra={"source_path": source_path, "operation": "freshness_trigger"},
        )


@retry_background_task(
    max_retries=RETRY_MAX_ATTEMPTS,
    initial_delay=RETRY_INITIAL_DELAY_FAST_SECONDS,
    backoff_multiplier=RETRY_BACKOFF_MULTIPLIER,
    operation_name="trigger_freshness_analysis",
    operation_type="freshness_analysis",
)
async def _trigger_freshness_analysis_background(source_path: str):
    """
    Background task to trigger freshness analysis without entity storage.

    Includes retry logic with exponential backoff:
    - Max retries: 3
    - Initial delay: 0.5s (faster for lightweight operation)
    - Backoff multiplier: 2x (0.5s â†’ 1s â†’ 2s)
    - Tracks metrics via Prometheus

    Args:
        source_path: Path of the document to analyze

    Raises:
        Exception: Propagated after all retries exhausted (for metrics tracking)
    """
    if not freshness_event_coordinator:
        logger.warning(
            "âš ï¸ Freshness event coordinator not available - skipping analysis",
            extra={"source_path": source_path},
        )
        return

    event = DocumentUpdateEvent(
        event_type=DocumentUpdateType.UPDATED,
        document_path=source_path,
        priority=EVENT_PRIORITY_NORMAL,
        requires_immediate_analysis=False,  # Use batch processing
        created_by="intelligence_service_analysis_only",
    )
    await freshness_event_coordinator.handle_document_update_event(event)
    logger.info(
        f"âœ… Triggered standalone freshness analysis for document: {source_path}",
        extra={"source_path": source_path, "operation": "freshness_trigger"},
    )


def _generate_document_id(
    url: Optional[str] = None,
    source_path: Optional[str] = None,
    title: Optional[str] = None,
    content: Optional[str] = None,
    project_id: Optional[str] = None,
    hash_length: int = DOCUMENT_ID_HASH_LENGTH,
) -> str:
    """
    Generate a unique document identifier using hash-based strategy.

    This function consolidates all document ID generation logic with a clear
    prioritization strategy:
    1. URL (highest priority - stable external identifier)
    2. Source path (stable file system identifier)
    3. Title + content preview (fallback for generated documents)

    Args:
        url: Optional URL of the document (highest priority)
        source_path: Optional file system path (medium priority)
        title: Optional document title (fallback, combined with content)
        content: Optional document content (fallback, first 100 chars used)
        project_id: Optional project ID for scoping (future use)
        hash_length: Length of hash substring (default: 12 for good uniqueness)

    Returns:
        Document ID in format "doc_{hash}" where hash is a stable identifier
        derived from the input parameters following the priority order.

    Examples:
        >>> _generate_document_id(url="https://example.com/doc")
        'doc_a1b2c3d4e5f6'

        >>> _generate_document_id(source_path="/path/to/file.md")
        'doc_7g8h9i0j1k2l'

        >>> _generate_document_id(title="My Doc", content="Some content...")
        'doc_3m4n5o6p7q8r'

    Raises:
        ValueError: If no valid input is provided to generate ID
    """
    # Priority 1: URL (most stable external identifier)
    if url:
        hash_input = url
    # Priority 2: Source path (stable file system identifier)
    elif source_path:
        hash_input = source_path
    # Priority 3: Title + content preview (fallback)
    elif title or content:
        # Combine title and content preview for uniqueness
        title_part = title or ""
        content_part = (content or "")[:DOCUMENT_CONTENT_PREVIEW_LENGTH]
        hash_input = title_part + content_part
    else:
        raise ValueError(
            "At least one of url, source_path, title, or content must be provided "
            "to generate document ID"
        )

    # Generate stable hash-based ID using BLAKE2b
    # BLAKE2b provides cryptographically secure, deterministic hashing
    # (unlike Python's hash() which is randomized per process for security)
    hash_value = hashlib.blake2b(
        hash_input.encode("utf-8"), digest_size=16
    ).hexdigest()[:hash_length]
    document_id = f"doc_{hash_value}"

    return document_id


def _map_extension_to_language(file_extension: str) -> str:
    """
    Map file extension to language identifier.

    Args:
        file_extension: File extension (with or without leading dot)

    Returns:
        Language identifier string (e.g., 'python', 'javascript')

    Examples:
        >>> _map_extension_to_language(".py")
        'python'
        >>> _map_extension_to_language("js")
        'javascript'
    """
    # Extension to language mapping
    # This mapping aligns with the BaseEntityExtractor.supported_languages
    extension_map = {
        "py": "python",
        "js": "javascript",
        "ts": "typescript",
        "tsx": "typescript",
        "jsx": "javascript",
        "go": "go",
        "java": "java",
        "rs": "rust",
        "cpp": "cpp",
        "cc": "cpp",
        "cxx": "cpp",
        "c": "c",
        "h": "c",
        "hpp": "cpp",
        "rb": "ruby",
        "php": "php",
        "swift": "swift",
        "kt": "kotlin",
        "scala": "scala",
        "cs": "csharp",
        "vb": "vb",
        "sql": "sql",
        "sh": "shell",
        "bash": "shell",
        "zsh": "shell",
        "yaml": "yaml",
        "yml": "yaml",
        "json": "json",
        "xml": "xml",
        "html": "html",
        "css": "css",
        "scss": "scss",
        "sass": "sass",
        "md": "markdown",
        "rst": "restructuredtext",
        "txt": "text",
    }

    # Clean extension (remove leading dot and convert to lowercase)
    clean_ext = file_extension.lstrip(".").lower()

    # Return mapped language or the extension itself if not found
    return extension_map.get(clean_ext, clean_ext)


def _enhance_document_metadata(
    base_metadata: dict,
    document_id: str,
    title: str,
    source: str,
    project_id: Optional[str] = None,
    document_type: Optional[str] = None,
    source_domain: Optional[str] = None,
    quality_scoring_enabled: Optional[bool] = None,
    timestamp_field: str = "created_at",
) -> dict:
    """
    Enhance document metadata with common fields.

    Args:
        base_metadata: Original metadata dictionary to extend
        document_id: Unique document identifier
        title: Document title
        source: Source identifier (e.g., 'archon_document', 'batch_indexing')
        project_id: Optional project identifier
        document_type: Optional document type
        source_domain: Optional source domain for batch indexing
        quality_scoring_enabled: Optional quality scoring flag
        timestamp_field: Name for timestamp field (default: 'created_at')

    Returns:
        Enhanced metadata dictionary with all provided fields

    Note:
        - If base_metadata contains 'file_extension' but not 'language',
          automatically adds 'language' field by mapping the file extension
          to its corresponding language identifier (e.g., '.py' -> 'python')

    Example:
        enhanced = _enhance_document_metadata(
            base_metadata={"custom": "field", "file_extension": ".py"},
            document_id="doc_123",
            title="My Document",
            source="archon_document",
            project_id="proj_456",
            document_type="spec"
        )
        # enhanced["language"] will be "python"
    """
    enhanced = {
        **base_metadata,
        "document_id": document_id,
        "title": title,
        "source": source,
        timestamp_field: datetime.now(timezone.utc).isoformat(),
    }

    # Add optional fields only if provided
    if project_id is not None:
        enhanced["project_id"] = project_id
    if document_type is not None:
        enhanced["document_type"] = document_type
    if source_domain is not None:
        enhanced["source_domain"] = source_domain
    if quality_scoring_enabled is not None:
        enhanced["quality_scoring_enabled"] = quality_scoring_enabled

    # Add language field based on file_extension if present and language not already set
    if "file_extension" in enhanced and "language" not in enhanced:
        enhanced["language"] = _map_extension_to_language(enhanced["file_extension"])

    return enhanced


# Performance Optimization Endpoints (Phase 5A)


class OptimizationRequest(BaseModel):
    operation_name: str = Field(..., description="Name of operation to optimize")
    category: str = Field(..., description="Optimization category")
    test_duration_minutes: int = Field(
        default=5, description="Test duration in minutes"
    )


class BaselineRequest(BaseModel):
    operation_name: str = Field(..., description="Name of operation to baseline")
    code_content: Optional[str] = Field(
        default=None, description="Optional code content for immediate analysis"
    )


@app.post("/performance/baseline")
async def establish_performance_baseline(request: BaselineRequest):
    """
    Establish performance baseline for an operation.

    This endpoint now uses the fast PerformanceBaselineService instead of the old
    PerformanceOptimizer. Returns immediately with current baseline statistics
    calculated from existing measurements.

    If code_content is provided, performs immediate code analysis and returns
    synthetic baseline metrics. Otherwise, returns cached baseline from service.

    Performance: <100ms response time (was >10 minutes with old implementation)
    """
    try:
        if not performance_baseline_service:
            raise HTTPException(
                status_code=503, detail="Performance baseline service not initialized"
            )

        # If code content provided, perform immediate analysis
        if request.code_content:
            # Use quality scorer for immediate code analysis
            quality_scorer = QualityScorer()
            from models.entity_models import EntityMetadata, EntityType, KnowledgeEntity

            dummy_entity = KnowledgeEntity(
                entity_id=f"baseline_{request.operation_name}",
                name=request.operation_name,
                entity_type=EntityType.FUNCTION,
                description="Performance baseline analysis",
                source_path=f"{request.operation_name}.py",
                metadata=EntityMetadata(),
            )

            # Analyze code quality (fast operation, <50ms)
            quality_result = quality_scorer.score_entity(
                dummy_entity, request.code_content
            )

            # Generate synthetic baseline metrics based on code complexity
            # Higher complexity = higher response time estimate
            complexity_factor = 1.0 - quality_result.complexity_score
            base_response_time = 100 + (complexity_factor * 400)  # 100-500ms range

            return {
                "operation_name": request.operation_name,
                "average_response_time_ms": round(base_response_time, 2),
                "p50_ms": round(base_response_time * 0.9, 2),
                "p95_ms": round(base_response_time * 1.5, 2),
                "p99_ms": round(base_response_time * 2.0, 2),
                "quality_score": quality_result.overall_score,
                "complexity_score": quality_result.complexity_score,
                "sample_size": 1,
                "timestamp": datetime.now(timezone.utc).isoformat(),
                "source": "code_analysis",
                "message": "Synthetic baseline generated from code analysis",
            }

        # Otherwise, get cached baseline from service
        baseline = await performance_baseline_service.get_baseline(
            request.operation_name
        )

        if not baseline:
            # No baseline exists yet - return empty baseline
            return {
                "operation_name": request.operation_name,
                "message": "No baseline established yet. Measurements will be collected as operations execute.",
                "average_response_time_ms": 0.0,
                "p50_ms": 0.0,
                "p95_ms": 0.0,
                "p99_ms": 0.0,
                "sample_size": 0,
                "timestamp": datetime.now(timezone.utc).isoformat(),
                "source": "cached",
            }

        # Return existing baseline immediately
        return {
            "operation_name": request.operation_name,
            "average_response_time_ms": baseline["mean"],
            "p50_ms": baseline["p50"],
            "p95_ms": baseline["p95"],
            "p99_ms": baseline["p99"],
            "std_dev_ms": baseline["std_dev"],
            "sample_size": baseline["sample_size"],
            "timestamp": datetime.now(timezone.utc).isoformat(),
            "source": "cached",
            "message": f"Baseline calculated from {baseline['sample_size']} measurements",
        }

    except Exception as e:
        logger.error(f"Failed to get baseline: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@app.get("/performance/opportunities/{operation_name}")
async def get_optimization_opportunities(operation_name: str):
    """Get optimization opportunities for an operation"""
    try:
        if not performance_optimizer:
            raise HTTPException(
                status_code=503, detail="Performance optimizer not initialized"
            )

        opportunities = await performance_optimizer.identify_optimization_opportunities(
            operation_name=operation_name
        )

        return {"opportunities": opportunities}

    except ValueError as e:
        raise HTTPException(status_code=404, detail=str(e))
    except Exception as e:
        logger.error(f"Failed to get optimization opportunities: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@app.post("/performance/optimize")
async def apply_optimization(request: OptimizationRequest):
    """Apply an optimization and measure its impact"""
    try:
        if not performance_optimizer:
            raise HTTPException(
                status_code=503, detail="Performance optimizer not initialized"
            )

        # Convert category string to enum
        try:
            optimization_category = OptimizationCategory(request.category)
        except ValueError:
            raise HTTPException(
                status_code=400,
                detail=f"Invalid optimization category. Valid options: {[cat.value for cat in OptimizationCategory]}",
            )

        result = await performance_optimizer.apply_optimization(
            operation_name=request.operation_name,
            optimization_category=optimization_category,
            test_duration_minutes=request.test_duration_minutes,
        )

        return {
            "optimization_id": result.optimization_id,
            "success": result.success,
            "improvement_percentage": result.improvement_percentage,
            "cost_benefit_ratio": result.cost_benefit_ratio,
            "risk_assessment": result.risk_assessment,
            "description": result.description,
            "baseline_response_time_ms": result.baseline_metrics.average_response_time_ms,
            "optimized_response_time_ms": result.optimized_metrics.average_response_time_ms,
            "baseline_throughput": result.baseline_metrics.throughput_ops_per_sec,
            "optimized_throughput": result.optimized_metrics.throughput_ops_per_sec,
        }

    except ValueError as e:
        raise HTTPException(status_code=404, detail=str(e))
    except Exception as e:
        logger.error(f"Failed to apply optimization: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@app.get("/performance/report")
async def get_optimization_report():
    """Get comprehensive optimization report"""
    try:
        if not performance_optimizer:
            raise HTTPException(
                status_code=503, detail="Performance optimizer not initialized"
            )

        report = await performance_optimizer.generate_optimization_report()
        return report

    except Exception as e:
        logger.error(f"Failed to generate optimization report: {e}")
        raise HTTPException(status_code=500, detail=str(e))


# Phase 5D: Document Freshness Endpoints


@app.post("/freshness/analyze", response_model=FreshnessAnalysis)
async def analyze_document_freshness(
    request: FreshnessAnalysisRequest, background_tasks: BackgroundTasks
) -> FreshnessAnalysis:
    """Analyze document freshness for a path (file or directory)"""
    try:
        if not freshness_monitor:
            raise HTTPException(
                status_code=503, detail="Freshness monitor not initialized"
            )

        if Path(request.path).is_file():
            # Analyze single document
            document_freshness = await freshness_monitor.analyze_document(
                file_path=request.path,
                include_dependencies=request.calculate_dependencies,
            )

            # Create analysis result with single document
            analysis = FreshnessAnalysis(
                analysis_id=f"single_doc_{int(datetime.now(timezone.utc).timestamp())}",
                base_path=request.path,
                total_documents=1,
                analyzed_documents=1,
                skipped_documents=0,
                documents=[document_freshness],
                average_freshness_score=document_freshness.freshness_score.overall_score,
                stale_documents_count=1 if document_freshness.is_stale else 0,
                critical_documents_count=1 if document_freshness.is_critical else 0,
            )

        else:
            # Analyze directory
            analysis = await freshness_monitor.analyze_directory(
                directory_path=request.path,
                recursive=request.recursive,
                include_patterns=request.include_patterns,
                exclude_patterns=request.exclude_patterns,
                max_files=request.max_files,
            )

        # Store analysis in database in background
        if freshness_database:
            background_tasks.add_task(freshness_database.store_analysis, analysis)

        return analysis

    except Exception as e:
        logger.error(f"Document freshness analysis failed: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@app.get("/freshness/stale")
async def get_stale_documents(
    limit: Optional[int] = QUERY_LIMIT_LARGE,
    freshness_levels: Optional[str] = None,  # Comma-separated: STALE,OUTDATED,CRITICAL
    priority_filter: Optional[str] = None,  # LOW,MEDIUM,HIGH,CRITICAL
    document_types: Optional[str] = None,  # Comma-separated document types
    max_age_days: Optional[int] = None,
):
    """Get stale documents with filtering options"""
    try:
        if not freshness_database:
            raise HTTPException(
                status_code=503, detail="Freshness database not initialized"
            )

        # Parse filters
        levels = None
        if freshness_levels:
            levels = [
                FreshnessLevel(level.strip()) for level in freshness_levels.split(",")
            ]

        priority = None
        if priority_filter:
            priority = RefreshPriority(priority_filter)

        types = None
        if document_types:
            types = [doc_type.strip() for doc_type in document_types.split(",")]

        stale_docs = await freshness_database.get_stale_documents(
            limit=limit,
            freshness_levels=levels,
            priority_filter=priority,
            document_types=types,
            max_age_days=max_age_days,
        )

        return {"stale_documents": stale_docs, "count": len(stale_docs)}

    except Exception as e:
        logger.error(f"Failed to get stale documents: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@app.post("/freshness/refresh", response_model=RefreshResult)
async def refresh_stale_documents(
    request: RefreshRequest, background_tasks: BackgroundTasks
) -> RefreshResult:
    """Refresh stale documents with risk assessment and batch processing"""
    try:
        if not freshness_worker:
            raise HTTPException(
                status_code=503, detail="Freshness worker not initialized"
            )

        # Validate document paths
        missing_paths = []
        for path in request.document_paths:
            if not Path(path).exists():
                missing_paths.append(path)

        if missing_paths:
            raise HTTPException(
                status_code=400, detail=f"Missing document paths: {missing_paths}"
            )

        # Define progress callback
        progress_updates = []

        async def progress_callback(progress: float, message: str):
            progress_updates.append(
                {
                    "progress": progress,
                    "message": message,
                    "timestamp": datetime.now(timezone.utc),
                }
            )
            logger.info(f"Refresh progress: {progress:.1%} - {message}")

        # Execute refresh operation
        result = await freshness_worker.refresh_documents(
            request=request, progress_callback=progress_callback
        )

        # Add progress updates to result metadata
        if hasattr(result, "metadata"):
            result.metadata["progress_updates"] = progress_updates

        return result

    except Exception as e:
        logger.error(f"Document refresh failed: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@app.get("/freshness/stats", response_model=FreshnessStats)
async def get_freshness_statistics(base_path: Optional[str] = None):
    """Get comprehensive freshness statistics"""
    try:
        if not freshness_database:
            raise HTTPException(
                status_code=503, detail="Freshness database not initialized"
            )

        stats = await freshness_database.get_freshness_stats(base_path=base_path)
        return stats

    except Exception as e:
        logger.error(f"Failed to get freshness statistics: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@app.get("/freshness/analyses")
async def get_recent_analyses(limit: int = QUERY_LIMIT_SMALL):
    """Get recent freshness analyses"""
    try:
        if not freshness_database:
            raise HTTPException(
                status_code=503, detail="Freshness database not initialized"
            )

        analyses = await freshness_database.get_recent_analyses(limit=limit)
        return {"analyses": analyses, "count": len(analyses)}

    except Exception as e:
        logger.error(f"Failed to get recent analyses: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@app.get("/freshness/document/{path:path}")
async def get_document_freshness(path: str):
    """Get freshness information for a specific document"""
    try:
        if not freshness_database:
            raise HTTPException(
                status_code=503, detail="Freshness database not initialized"
            )

        # Decode path parameter
        document_path = "/" + path if not path.startswith("/") else path

        document_info = await freshness_database.get_document_by_path(document_path)

        if not document_info:
            raise HTTPException(
                status_code=404, detail=f"Document not found: {document_path}"
            )

        return document_info

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Failed to get document freshness: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@app.delete("/freshness/cleanup")
async def cleanup_old_freshness_data(days_to_keep: int = DATA_RETENTION_DAYS_DEFAULT):
    """Clean up old freshness data to prevent database bloat"""
    try:
        if not freshness_database:
            raise HTTPException(
                status_code=503, detail="Freshness database not initialized"
            )

        if days_to_keep < DATA_RETENTION_DAYS_MINIMUM:
            raise HTTPException(
                status_code=400,
                detail=f"days_to_keep must be at least {DATA_RETENTION_DAYS_MINIMUM}",
            )

        deleted_count = await freshness_database.cleanup_old_data(
            days_to_keep=days_to_keep
        )

        # Also cleanup old backups
        if freshness_worker:
            backup_deleted = await freshness_worker.cleanup_old_backups(
                days_to_keep=days_to_keep
            )
        else:
            backup_deleted = 0

        return {
            "deleted_database_records": deleted_count,
            "deleted_backup_directories": backup_deleted,
            "days_kept": days_to_keep,
        }

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Failed to cleanup old data: {e}")
        raise HTTPException(status_code=500, detail=str(e))


# Quality Assessment Endpoints (Phase 5A)


class CodeAssessmentRequest(BaseModel):
    content: str = Field(..., description="Code content to assess")
    source_path: str = Field(default="", description="Path to source file")
    language: str = Field(default="python", description="Programming language")
    include_patterns: bool = Field(default=True, description="Include pattern analysis")
    include_compliance: bool = Field(
        default=True, description="Include compliance check"
    )


class DocumentAssessmentRequest(BaseModel):
    content: str = Field(..., description="Document content to analyze")
    document_type: str = Field(default="markdown", description="Type of document")
    check_completeness: bool = Field(default=True, description="Check completeness")
    include_recommendations: bool = Field(
        default=True, description="Include recommendations"
    )


class PatternExtractionRequest(BaseModel):
    content: str = Field(..., description="Code content to analyze")
    pattern_type: str = Field(
        default="best_practices", description="Type of patterns to extract"
    )
    include_examples: bool = Field(default=True, description="Include examples")


class ComplianceCheckRequest(BaseModel):
    content: str = Field(..., description="Code content to check")
    architecture_type: str = Field(default="onex", description="Architecture type")
    include_violations: bool = Field(default=True, description="Include violations")
    include_recommendations: bool = Field(
        default=True, description="Include recommendations"
    )


# Tree Visualization Models


class TreeNodeMetadata(BaseModel):
    """Metadata for tree nodes"""

    file_size: Optional[int] = Field(None, description="File size in bytes")
    language: Optional[str] = Field(None, description="Programming language")
    entity_count: Optional[int] = Field(None, description="Number of entities")
    line_count: Optional[int] = Field(None, description="Number of lines")
    last_modified: Optional[str] = Field(
        None, description="Last modification timestamp"
    )
    indexed_at: Optional[str] = Field(None, description="Indexing timestamp")


class TreeNode(BaseModel):
    """Recursive tree node structure"""

    id: str = Field(..., description="Node identifier (entity_id)")
    name: str = Field(..., description="Node name")
    type: str = Field(..., description="Node type (project/directory/file)")
    path: str = Field(..., description="Full path")
    children: List["TreeNode"] = Field(default_factory=list, description="Child nodes")
    metadata: Optional[TreeNodeMetadata] = Field(None, description="Node metadata")


class DependencyEdge(BaseModel):
    """Import dependency edge"""

    source: str = Field(..., description="Source file entity_id")
    target: str = Field(..., description="Target file entity_id")
    type: str = Field(..., description="Dependency type (e.g., 'imports')")
    import_type: Optional[str] = Field(None, description="Import statement type")
    confidence: Optional[float] = Field(None, description="Relationship confidence")


class TreeStatistics(BaseModel):
    """Tree visualization statistics"""

    total_nodes: int = Field(0, description="Total nodes in tree")
    projects: int = Field(0, description="Number of projects")
    directories: int = Field(0, description="Number of directories")
    files: int = Field(0, description="Number of files")
    dependencies: int = Field(0, description="Number of dependencies")
    max_depth: int = Field(0, description="Maximum tree depth")


class TreeVisualization(BaseModel):
    """Complete tree visualization response"""

    tree: TreeNode = Field(..., description="Hierarchical tree structure")
    dependencies: List[DependencyEdge] = Field(
        default_factory=list, description="Dependency edges"
    )
    statistics: TreeStatistics = Field(..., description="Tree statistics")
    project_name: str = Field(..., description="Project name")


# Enable forward references for recursive TreeNode
TreeNode.model_rebuild()


@app.post("/assess/code")
async def assess_code_quality(request: CodeAssessmentRequest):
    """Assess code quality with ONEX architectural compliance scoring"""
    try:
        if not intelligence_service:
            raise HTTPException(
                status_code=503, detail="Intelligence service not initialized"
            )

        # Use the quality scorer to assess the code
        quality_scorer = QualityScorer()

        # Create a dummy entity for scoring (since QualityScorer expects KnowledgeEntity)
        from models.entity_models import EntityMetadata, EntityType, KnowledgeEntity

        dummy_entity = KnowledgeEntity(
            entity_id="temp_assessment",
            name="code_assessment",
            entity_type=EntityType.FUNCTION,
            description="Temporary entity for code quality assessment",
            source_path=request.source_path or "temp_assessment.py",
            metadata=EntityMetadata(),
        )

        # Assess code quality using actual QualityScorer methods
        quality_result = quality_scorer.score_entity(dummy_entity, request.content)

        # Extract patterns if requested
        patterns = (
            quality_scorer.detect_patterns(request.content, dummy_entity)
            if request.include_patterns
            else []
        )

        # Calculate architectural compliance directly
        compliance_score, compliance_reason = (
            quality_scorer._calculate_architectural_compliance(
                dummy_entity, request.content
            )
            if request.include_compliance
            else (0.8, "No compliance check")
        )

        return {
            "success": True,
            "quality_score": quality_result.overall_score,
            "architectural_compliance": {
                "score": compliance_score,
                "reasoning": compliance_reason,
            },
            "code_patterns": [
                {
                    "pattern_name": p.pattern_name,
                    "pattern_type": p.pattern_type,
                    "confidence": p.confidence,
                    "description": p.description,
                    "severity": p.severity,
                    "recommendation": p.recommendation,
                }
                for p in patterns
            ],
            "maintainability": {
                "complexity_score": quality_result.complexity_score,
                "readability_score": quality_result.maintainability_score,
                "testability_score": quality_result.maintainability_score,
            },
            "onex_compliance": {
                "score": compliance_score,
                "violations": [],
                "recommendations": [compliance_reason],
            },
            "architectural_era": "modern",
            "temporal_relevance": quality_result.temporal_relevance,
            "timestamp": datetime.now(timezone.utc).isoformat(),
        }

    except Exception as e:
        logger.error(f"Code quality assessment failed: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@app.post("/assess/document")
async def assess_document_quality(request: DocumentAssessmentRequest):
    """Analyze document quality and completeness"""
    try:
        if not intelligence_service:
            raise HTTPException(
                status_code=503, detail="Intelligence service not initialized"
            )

        # Use quality scorer for document analysis (simplified approach)
        QualityScorer()

        # Create dummy entity for document analysis
        from models.entity_models import EntityMetadata, EntityType, KnowledgeEntity

        KnowledgeEntity(
            entity_id="temp_doc_assessment",
            name="document_assessment",
            entity_type=EntityType.DOCUMENT,
            description="Temporary entity for document quality assessment",
            source_path="temp_document.md",
            metadata=EntityMetadata(),
        )

        # Get basic document quality metrics
        word_count = len(request.content.split())
        section_count = len(request.content.split("\n#"))
        has_headings = bool(request.content.count("#"))

        # Simple quality calculation based on structure
        structure_score = (
            min(1.0, (section_count - 1) * 0.2) if section_count > 1 else 0.3
        )
        content_score = (
            min(1.0, word_count / float(DOCUMENT_QUALITY_WORD_COUNT_THRESHOLD))
            if word_count < DOCUMENT_QUALITY_WORD_COUNT_THRESHOLD
            else 1.0
        )
        overall_score = (structure_score + content_score) / 2

        return {
            "success": True,
            "quality_score": overall_score,
            "completeness": (
                {
                    "has_structure": has_headings,
                    "word_count": word_count,
                    "section_count": section_count,
                    "completeness_score": overall_score,
                }
                if request.check_completeness
                else {}
            ),
            "clarity": {
                "readability_score": content_score,
                "structure_score": structure_score,
                "consistency_score": 0.8,
            },
            "recommendations": [
                "Add more sections" if section_count <= 2 else "Good structure",
                "Add more content" if word_count < 200 else "Good content length",
            ],
            "freshness_analysis": {"last_updated": "recent", "relevance_score": 0.9},
            "structure_analysis": {
                "sections_count": section_count,
                "word_count": word_count,
                "structure_quality": (
                    "good" if structure_score > 0.6 else "needs improvement"
                ),
            },
            "timestamp": datetime.now(timezone.utc).isoformat(),
        }

    except Exception as e:
        logger.error(f"Document quality assessment failed: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@app.post("/patterns/extract")
async def extract_quality_patterns(request: PatternExtractionRequest):
    """Extract quality patterns and anti-patterns from code"""
    try:
        if not intelligence_service:
            raise HTTPException(
                status_code=503, detail="Intelligence service not initialized"
            )

        # Use quality scorer for pattern extraction
        quality_scorer = QualityScorer()

        # Create dummy entity for pattern analysis
        from models.entity_models import EntityMetadata, EntityType, KnowledgeEntity

        dummy_entity = KnowledgeEntity(
            entity_id="temp_pattern_assessment",
            name="pattern_assessment",
            entity_type=EntityType.FUNCTION,
            description="Temporary entity for pattern analysis",
            source_path="temp_pattern.py",
            metadata=EntityMetadata(),
        )

        # Extract patterns using actual QualityScorer method
        patterns = quality_scorer.detect_patterns(request.content, dummy_entity)

        return {
            "success": True,
            "analysis": {
                "pattern_type": request.pattern_type,
                "content_analyzed": len(request.content),
                "patterns_found": len(patterns),
            },
            "patterns": [
                {
                    "pattern_name": p.pattern_name,
                    "pattern_type": p.pattern_type,
                    "confidence": p.confidence,
                    "description": p.description,
                    "severity": p.severity,
                    "recommendation": p.recommendation,
                    "location": getattr(p, "location", {}),
                }
                for p in patterns
            ],
            "confidence_scores": {
                "overall_confidence": (
                    sum(p.confidence for p in patterns) / len(patterns)
                    if patterns
                    else 0.8
                ),
                "pattern_accuracy": 0.85,
            },
            "recommendations": [
                p.recommendation for p in patterns if hasattr(p, "recommendation")
            ],
            "timestamp": datetime.now(timezone.utc).isoformat(),
        }

    except Exception as e:
        logger.error(f"Pattern extraction failed: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@app.post("/compliance/check")
async def check_architectural_compliance(request: ComplianceCheckRequest):
    """Check architectural compliance against ONEX standards"""
    try:
        if not intelligence_service:
            raise HTTPException(
                status_code=503, detail="Intelligence service not initialized"
            )

        # Validate request content
        if not request.content:
            return {
                "success": False,
                "error": "No content provided for compliance check",
                "compliance_score": 0.0,
                "violations": [],
                "recommendations": ["Provide code content for analysis"],
                "architectural_era": "unknown",
                "pattern_analysis": {"design_patterns": [], "anti_patterns": []},
                "quality_gates": {
                    "passed": False,
                    "score": 0.0,
                    "threshold": 0.7,
                },
                "timestamp": datetime.now(timezone.utc).isoformat(),
            }

        # Use quality scorer for compliance check
        quality_scorer = QualityScorer()

        # Create dummy entity for compliance analysis
        from models.entity_models import EntityMetadata, EntityType, KnowledgeEntity

        dummy_entity = KnowledgeEntity(
            entity_id="temp_compliance_assessment",
            name="compliance_assessment",
            entity_type=EntityType.FUNCTION,
            description="Temporary entity for compliance analysis",
            source_path="temp_compliance.py",
            metadata=EntityMetadata(),
        )

        # Check architectural compliance using actual method with null safety
        result = quality_scorer._calculate_architectural_compliance(
            dummy_entity, request.content
        )

        # Null safety: ensure result is a tuple
        if not isinstance(result, tuple) or len(result) != 2:
            logger.error(
                f"Invalid result from _calculate_architectural_compliance: {result}"
            )
            raise ValueError(f"Invalid compliance check result: {type(result)}")

        compliance_score, compliance_reason = result

        # Null safety: ensure compliance_score is a number
        if compliance_score is None or not isinstance(compliance_score, (int, float)):
            logger.error(f"Invalid compliance_score: {compliance_score}")
            compliance_score = 0.5

        # Null safety: ensure compliance_reason is a string
        if compliance_reason is None:
            compliance_reason = "Unknown compliance status"

        return {
            "success": True,
            "compliance_score": compliance_score,
            "violations": [] if request.include_violations else [],
            "recommendations": (
                [compliance_reason] if request.include_recommendations else []
            ),
            "architectural_era": "modern",
            "pattern_analysis": {"design_patterns": [], "anti_patterns": []},
            "quality_gates": {
                "passed": compliance_score > CONFIDENCE_THRESHOLD_QUALITY,
                "score": compliance_score,
                "threshold": CONFIDENCE_THRESHOLD_QUALITY,
            },
            "timestamp": datetime.now(timezone.utc).isoformat(),
        }

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Compliance check failed: {e}", exc_info=True)
        # Return error response instead of raising exception
        return {
            "success": False,
            "error": str(e),
            "compliance_score": 0.0,
            "violations": [],
            "recommendations": [f"Error during compliance check: {str(e)}"],
            "architectural_era": "unknown",
            "pattern_analysis": {"design_patterns": [], "anti_patterns": []},
            "quality_gates": {
                "passed": False,
                "score": 0.0,
                "threshold": CONFIDENCE_THRESHOLD_QUALITY,
            },
            "timestamp": datetime.now(timezone.utc).isoformat(),
        }


# Real-time document indexing endpoint
@app.post("/process/document")
async def process_document_for_indexing(
    request: dict, background_tasks: BackgroundTasks
):
    """
    Process document for real-time vectorization and indexing.

    This endpoint is called by document creation hooks to immediately
    index new documents into the RAG system.

    Args:
        request: Document processing request with fields:
            - document_id: Unique document identifier
            - project_id: Associated project ID
            - title: Document title
            - content: Document content (text or structured)
            - document_type: Type of document (spec, design, note, etc.)
            - metadata: Additional metadata
    """
    try:
        if not intelligence_service:
            raise HTTPException(
                status_code=503, detail="Intelligence service not initialized"
            )

        # Extract request fields
        document_id = request.get("document_id")
        project_id = request.get("project_id")
        title = request.get("title", "")
        content = request.get("content", {})
        document_type = request.get("document_type", "document")
        metadata = request.get("metadata", {})

        if not document_id or not project_id:
            raise HTTPException(
                status_code=400, detail="document_id and project_id are required"
            )

        # Convert content to text for processing
        if isinstance(content, dict):
            # Extract text content from structured data
            content_text = ""
            if "text" in content:
                content_text = content["text"]
            elif "overview" in content:
                content_text = content["overview"]
            elif "description" in content:
                content_text = content["description"]
            else:
                # Flatten all string values
                content_text = " ".join(
                    str(value)
                    for value in content.values()
                    if isinstance(value, (str, int, float))
                )
        else:
            content_text = str(content)

        # Combine title and content for full text
        full_text = f"{title}\n\n{content_text}".strip()

        # Create source path identifier
        source_path = f"archon://projects/{project_id}/documents/{document_id}"

        # Prepare metadata for entity extraction
        enhanced_metadata = _enhance_document_metadata(
            base_metadata=metadata,
            document_id=document_id,
            title=title,
            source="archon_document",
            project_id=project_id,
            document_type=document_type,
            timestamp_field="created_at",
        )

        logger.info(
            f"ðŸ”¬ [INDEXING PIPELINE] Processing document for indexing | document_id={document_id} | "
            f"project_id={project_id} | content_length={len(full_text)} | source_path={source_path}"
        )

        # Extract entities and relationships from document content
        logger.info(
            f"ðŸ” [ENRICHMENT STEP] Extracting entities | document_id={document_id} | "
            f"content_length={len(full_text)} | source_path={source_path}"
        )
        entities, relationships_dict = (
            await intelligence_service.extract_entities_from_document(
                content=full_text, source_path=source_path, metadata=enhanced_metadata
            )
        )

        # Convert relationships to model objects
        relationships = _convert_relationships_to_models(relationships_dict)

        logger.info(
            f"âœ… [ENRICHMENT STEP] Entities and relationships extracted | document_id={document_id} | "
            f"entity_count={len(entities)} | "
            f"relationship_count={len(relationships)} | "
            f"entity_types={list(set(e.entity_type.value for e in entities))[:10] if entities else []}"
        )

        # Log what's about to be stored
        logger.info(
            f"ðŸ“¦ [STORAGE PREP] Preparing data for storage | document_id={document_id} | "
            f"entities_to_store={len(entities)} | "
            f"relationships_to_store={len(relationships)} | "
            f"document_length={len(full_text)} | "
            f"metadata_fields={list(enhanced_metadata.keys())}"
        )

        # Store entities, relationships, and trigger vector indexing in background
        background_tasks.add_task(
            _process_document_background,
            entities,
            source_path,
            full_text,
            enhanced_metadata,
            document_id,
            project_id,
            relationships,  # Pass relationships to background task
        )

        logger.info(
            f"âœ… [PIPELINE] Document processing queued for background storage | "
            f"document_id={document_id} | "
            f"entities_extracted={len(entities)} | "
            f"will_store_to_memgraph={memgraph_adapter is not None} | "
            f"will_vectorize={True}"
        )

        return {
            "success": True,
            "document_id": document_id,
            "project_id": project_id,
            "entities_extracted": len(entities),
            "status": "processing_queued",
            "status_url": f"/process/document/{document_id}/status",
            "message": "Document queued for vectorization and indexing. Check status_url for completion.",
        }

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Document processing failed: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@app.get("/process/document/{document_id}/status")
async def get_document_processing_status(document_id: str):
    """
    Get the status of a document processing background task.

    This endpoint allows clients to check if a document processing task has completed,
    is still running, or has failed. Useful for polling after queuing a document for processing.

    Args:
        document_id: Document identifier (same as used in /process/document request)

    Returns:
        Status information including:
        - status: "pending", "running", "success", or "failed"
        - started_at: When the task started (ISO timestamp)
        - completed_at: When the task completed (ISO timestamp, null if still running)
        - error_message: Error message if failed
        - entities_extracted: Number of entities extracted (if successful)
        - vector_indexed: Whether vector indexing succeeded (if successful)
        - pipeline_steps: Status of each pipeline step

    Example:
        GET /process/document/doc-123/status

        Response:
        {
            "document_id": "doc-123",
            "status": "success",
            "started_at": "2025-11-12T10:00:00Z",
            "completed_at": "2025-11-12T10:00:05Z",
            "entities_extracted": 42,
            "vector_indexed": true,
            "pipeline_steps": {
                "file_node_creation": "success",
                "memgraph_storage": "success",
                "embedding_generation": "success",
                "qdrant_indexing": "success",
                "freshness_analysis": "success"
            }
        }
    """
    tracker = get_global_tracker()
    if not tracker:
        raise HTTPException(
            status_code=503,
            detail="Background task tracking not available"
        )

    status = await tracker.get_status(document_id)

    if not status:
        raise HTTPException(
            status_code=404,
            detail=f"No status found for document_id: {document_id}"
        )

    return {
        "document_id": status.document_id,
        "correlation_id": status.correlation_id,
        "status": status.status.value,
        "started_at": status.started_at.isoformat() if status.started_at else None,
        "completed_at": status.completed_at.isoformat() if status.completed_at else None,
        "error_message": status.error_message,
        "error_details": status.error_details,
        "entities_extracted": status.entities_extracted,
        "vector_indexed": status.vector_indexed,
        "pipeline_steps": status.pipeline_steps,
    }


def _convert_relationships_to_models(
    relationships: List[dict],
) -> List[KnowledgeRelationship]:
    """
    Convert relationship dictionaries from LangExtract to KnowledgeRelationship objects.

    Args:
        relationships: List of relationship dictionaries from LangExtract service

    Returns:
        List of KnowledgeRelationship objects
    """
    converted = []
    for rel_dict in relationships:
        try:
            # Handle relationship_type - can be string or enum
            rel_type_value = rel_dict.get("relationship_type", "RELATES_TO")
            if isinstance(rel_type_value, str):
                try:
                    rel_type = RelationshipType(rel_type_value)
                except ValueError:
                    logger.warning(
                        f"Unknown relationship type '{rel_type_value}', defaulting to RELATES_TO"
                    )
                    rel_type = RelationshipType.RELATES_TO
            else:
                rel_type = rel_type_value

            # Create KnowledgeRelationship object
            # Generate relationship_id if not provided
            rel_id = rel_dict.get("relationship_id")
            if not rel_id:
                source_id = rel_dict.get("source_entity_id", "")
                target_id = rel_dict.get("target_entity_id", "")
                rel_id = f"rel_{hashlib.blake2b(f'{source_id}_{target_id}'.encode()).hexdigest()[:16]}"

            relationship = KnowledgeRelationship(
                relationship_id=rel_id,
                source_entity_id=rel_dict.get("source_entity_id", ""),
                target_entity_id=rel_dict.get("target_entity_id", ""),
                relationship_type=rel_type,
                confidence_score=float(rel_dict.get("confidence_score", 0.7)),
                properties=rel_dict.get("properties", {}),
                created_at=datetime.now(timezone.utc),
            )
            converted.append(relationship)
        except Exception as e:
            logger.warning(f"Failed to convert relationship: {e}, dict: {rel_dict}")
            continue

    return converted


def _create_file_node(
    source_path: str,
    full_text: str,
    metadata: dict,
    entities: List[KnowledgeEntity],
) -> dict:
    """
    Create file node data for Memgraph storage.

    Extracts file-level metadata from document processing pipeline.

    Args:
        source_path: Full or relative file path
        full_text: Document content
        metadata: Enhanced metadata dictionary
        entities: Extracted entities for counting

    Returns:
        Dictionary with file node properties
    """
    from pathlib import Path

    # Generate stable file ID based on path
    path_hash = hashlib.md5(source_path.encode()).hexdigest()[:12]
    file_id = f"file_{path_hash}"

    # Extract file name and path components
    path_obj = Path(source_path)
    filename = path_obj.name
    relative_path = str(path_obj)

    # Extract metadata fields (with defaults)
    project_name = metadata.get("project_name", metadata.get("project_id", "unknown"))
    language = metadata.get("language", metadata.get("file_type", "unknown"))
    file_hash = metadata.get(
        "file_hash", hashlib.md5(full_text.encode()).hexdigest()[:16]
    )

    # Calculate file metrics
    file_size = len(full_text.encode("utf-8"))
    line_count = len(full_text.splitlines())
    entity_count = len(entities)

    # Count imports/includes (simple heuristic)
    import_count = 0
    for line in full_text.splitlines()[:100]:  # Check first 100 lines
        line_lower = line.strip().lower()
        if any(
            keyword in line_lower
            for keyword in ["import ", "from ", "#include", "require("]
        ):
            import_count += 1

    # Determine content type
    content_type = (
        "code" if language not in ["markdown", "text", "unknown"] else "documentation"
    )

    file_data = {
        "entity_id": file_id,
        "name": filename,
        "path": source_path,
        "relative_path": relative_path,
        "project_name": project_name,
        "file_size": file_size,
        "language": language,
        "file_hash": file_hash,
        "last_modified": metadata.get(
            "last_modified", datetime.now(timezone.utc).isoformat()
        ),
        "indexed_at": datetime.now(timezone.utc).isoformat(),
        "content_type": content_type,
        "line_count": line_count,
        "entity_count": entity_count,
        "import_count": import_count,
    }

    return file_data


async def _store_file_imports(
    source_file: str, import_relationships: List, project_id: str
) -> int:
    """
    Store file-to-file import relationships in Memgraph.

    Args:
        source_file: Source file entity_id (format: "file:{project_id}:{relative_path}")
        import_relationships: List of CodeRelationship objects with IMPORTS type
        project_id: Project identifier

    Returns:
        Number of relationships successfully created
    """
    if not memgraph_adapter or not import_relationships:
        return 0

    stored_count = 0

    for rel in import_relationships:
        try:
            # Resolve target file path
            # rel.target contains the import module/package path
            target_module = rel.target

            # For now, use simple target file ID construction
            # TODO: Enhance with actual file path resolution from module names
            target_file_id = f"file:{project_id}:{target_module}"

            # Create IMPORTS relationship in Memgraph
            success = await memgraph_adapter.create_file_import_relationship(
                source_id=source_file,
                target_id=target_file_id,
                import_type=rel.properties.get("import_type", "module"),
                confidence=rel.confidence,
            )

            if success:
                stored_count += 1
                logger.debug(
                    f"âœ… [IMPORT STORAGE] Created import relationship | "
                    f"source={source_file} | target={target_file_id} | type={rel.properties.get('import_type')}",
                    extra={
                        "source_file": source_file,
                        "target_file": target_file_id,
                        "import_type": rel.properties.get("import_type"),
                        "confidence": rel.confidence,
                    },
                )

        except Exception as e:
            logger.warning(
                f"âš ï¸ [IMPORT STORAGE] Failed to store import relationship | "
                f"source={source_file} | target={rel.target} | error={str(e)}",
                extra={
                    "source_file": source_file,
                    "target_module": rel.target,
                    "error": str(e),
                },
            )

    return stored_count


@retry_background_task(
    max_retries=RETRY_MAX_ATTEMPTS,
    initial_delay=RETRY_INITIAL_DELAY_SLOW_SECONDS,
    backoff_multiplier=RETRY_BACKOFF_MULTIPLIER,
    operation_name="process_document",
    operation_type="document_processing",
)
async def _process_document_background(
    entities: List[KnowledgeEntity],
    source_path: str,
    full_text: str,
    metadata: dict,
    document_id: str,
    project_id: str,
    relationships: Optional[List[KnowledgeRelationship]] = None,
):
    """
    Background task to store entities, relationships, and trigger vector indexing.

    This coordinates with the search service to ensure documents
    are immediately available in RAG queries.

    Includes retry logic with exponential backoff:
    - Max retries: 3
    - Initial delay: 2s (longer for complex multi-step operation)
    - Backoff multiplier: 2x (2s â†’ 4s â†’ 8s)
    - Tracks metrics via Prometheus
    - Handles partial failures gracefully

    Pipeline Steps:
    0. Create file node in Memgraph knowledge graph
    1. Store entities in Memgraph knowledge graph
    2. Store relationships in Memgraph knowledge graph
    3. Vectorize document via search service
    4. Trigger freshness analysis

    Args:
        entities: List of extracted knowledge entities
        source_path: Source path of the document
        full_text: Full document text content
        metadata: Document metadata dictionary
        document_id: Unique document identifier
        project_id: Associated project ID
        relationships: Optional list of relationships between entities

    Raises:
        Exception: Propagated after all retries exhausted (for metrics tracking)
    """
    # Track background task status
    tracker = get_global_tracker()
    if tracker:
        await tracker.start_task(document_id, correlation_id=None)  # TODO: Add correlation ID propagation

    try:
        # Pipeline Stage Logging: Track pipeline execution for debugging
        logger.info(
            f"[PIPELINE] Stage 1: Creating file node | document_id={document_id} | source_path={source_path}",
            extra={"document_id": document_id, "pipeline_stage": 1, "stage_name": "file_node_creation"}
        )

        # Step 0: Create file node in Memgraph knowledge graph
        if memgraph_adapter:
            try:
                file_data = _create_file_node(source_path, full_text, metadata, entities)
                logger.info(
                    f"ðŸ“ [INDEXING PIPELINE] Creating file node | document_id={document_id} | "
                    f"file_path={source_path} | language={file_data['language']} | "
                    f"line_count={file_data['line_count']} | entity_count={file_data['entity_count']}",
                    extra={
                        "document_id": document_id,
                        "project_id": project_id,
                        "file_path": source_path,
                        "pipeline_step": "file_node_creation",
                    },
                )
                success = await memgraph_adapter.create_file_node(file_data)
                if success:
                    logger.info(
                        f"âœ… [INDEXING PIPELINE] File node created | document_id={document_id} | "
                        f"file_id={file_data['entity_id']} | file_path={source_path}",
                        extra={
                            "document_id": document_id,
                            "file_id": file_data["entity_id"],
                            "pipeline_step": "file_node_creation",
                            "status": "success",
                        },
                    )
                else:
                    logger.warning(
                        f"âš ï¸ [INDEXING PIPELINE] File node creation returned false | document_id={document_id} | "
                        f"file_path={source_path}",
                        extra={
                            "document_id": document_id,
                            "pipeline_step": "file_node_creation",
                            "status": "warning",
                        },
                    )
            except Exception as e:
                # Log error but continue pipeline - file nodes are supplementary
                logger.error(
                    f"âŒ [INDEXING PIPELINE] File node creation failed | document_id={document_id} | "
                    f"error={str(e)}",
                    extra={
                        "document_id": document_id,
                        "pipeline_step": "file_node_creation",
                        "status": "error",
                        "error": str(e),
                    },
                )

        # Pipeline Stage Logging
        logger.info(
            f"[PIPELINE] Stage 2: Storing entities | document_id={document_id} | entity_count={len(entities)}",
            extra={"document_id": document_id, "pipeline_stage": 2, "stage_name": "entity_storage"}
        )

        # Step 1: Store entities in Memgraph knowledge graph
        if memgraph_adapter and entities:
            logger.info(
                f"ðŸ”— [INDEXING PIPELINE] Storing entities in Memgraph | document_id={document_id} | entities_count={len(entities)}",
                extra={
                    "document_id": document_id,
                    "project_id": project_id,
                    "entities_count": len(entities),
                    "pipeline_step": "memgraph_storage",
                },
            )
            await memgraph_adapter.store_entities(entities)
            logger.info(
                f"âœ… [INDEXING PIPELINE] Entities stored in Memgraph | document_id={document_id}",
                extra={
                    "document_id": document_id,
                    "pipeline_step": "memgraph_storage",
                    "status": "success",
                },
            )

            # Step 1b: Store relationships in Memgraph if provided
            if relationships:
                logger.info(
                    f"ðŸ”— [INDEXING PIPELINE] Storing relationships in Memgraph | document_id={document_id} | relationships_count={len(relationships)}",
                    extra={
                        "document_id": document_id,
                        "project_id": project_id,
                        "relationships_count": len(relationships),
                        "pipeline_step": "memgraph_relationship_storage",
                    },
                )
                stored_count = await memgraph_adapter.store_relationships(relationships)
                logger.info(
                    f"âœ… [INDEXING PIPELINE] Relationships stored in Memgraph | document_id={document_id} | stored_count={stored_count}",
                    extra={
                        "document_id": document_id,
                        "pipeline_step": "memgraph_relationship_storage",
                        "status": "success",
                        "stored_count": stored_count,
                    },
                )
        else:
            logger.warning(
                f"âš ï¸ [INDEXING PIPELINE] Skipping Memgraph storage | document_id={document_id} | memgraph_available={memgraph_adapter is not None} | entities_count={len(entities)}",
                extra={
                    "document_id": document_id,
                    "pipeline_step": "memgraph_storage",
                    "status": "skipped",
                },
            )

        # Step 1c: Extract and store file-level imports
        if code_relationship_detector and memgraph_adapter:
            language = (
                metadata.get("file_extension", metadata.get("language", ""))
                .lower()
                .lstrip(".")
            )
            supported_languages = [
                "python",
                "py",
                "javascript",
                "js",
                "typescript",
                "ts",
                "go",
            ]

            if language in supported_languages:
                try:
                    logger.info(
                        f"ðŸ“¦ [IMPORT EXTRACTION] Extracting file imports | document_id={document_id} | language={language}",
                        extra={
                            "document_id": document_id,
                            "project_id": project_id,
                            "language": language,
                            "pipeline_step": "import_extraction",
                        },
                    )

                    # Extract all code relationships
                    file_relationships = (
                        await code_relationship_detector.detect_relationships(
                            content=full_text, language=language, document_path=source_path
                        )
                    )

                    # Filter for IMPORTS relationships only
                    import_relationships = [
                        rel
                        for rel in file_relationships
                        if rel.relationship_type == "IMPORTS"
                    ]

                    if import_relationships:
                        # Store file-to-file import relationships
                        stored_imports = await _store_file_imports(
                            source_file=f"file:{project_id}:{metadata.get('relative_path', source_path)}",
                            import_relationships=import_relationships,
                            project_id=project_id,
                        )

                        logger.info(
                            f"âœ… [IMPORT EXTRACTION] Extracted and stored imports | document_id={document_id} | "
                            f"imports_count={len(import_relationships)} | stored_count={stored_imports}",
                            extra={
                                "document_id": document_id,
                                "project_id": project_id,
                                "imports_count": len(import_relationships),
                                "stored_count": stored_imports,
                                "pipeline_step": "import_extraction",
                                "status": "success",
                            },
                        )
                    else:
                        logger.debug(
                            f"â„¹ï¸ [IMPORT EXTRACTION] No imports found | document_id={document_id}",
                            extra={
                                "document_id": document_id,
                                "pipeline_step": "import_extraction",
                            },
                        )

                except Exception as e:
                    # Log error but continue pipeline - import extraction is supplementary
                    logger.warning(
                        f"âš ï¸ [IMPORT EXTRACTION] Failed to extract imports | document_id={document_id} | error={str(e)}",
                        extra={
                            "document_id": document_id,
                            "pipeline_step": "import_extraction",
                            "status": "warning",
                            "error": str(e),
                        },
                    )
            else:
                logger.debug(
                    f"â„¹ï¸ [IMPORT EXTRACTION] Skipping unsupported language | document_id={document_id} | language={language}",
                    extra={
                        "document_id": document_id,
                        "language": language,
                        "pipeline_step": "import_extraction",
                    },
                )

        # Pipeline Stage Logging
        logger.info(
            f"[PIPELINE] Stage 3: Starting vectorization | document_id={document_id} | content_length={len(full_text)}",
            extra={"document_id": document_id, "pipeline_stage": 3, "stage_name": "vectorization"}
        )

        # Step 2: Generate embedding and index in Qdrant directly using QdrantIndexerEffect
        embeddings_generated = False
        vector_indexed = False

        try:
            # Generate embedding using vLLM service
            embedding_model_url = os.getenv(
                "EMBEDDING_MODEL_URL", "http://192.168.86.201:8002"
            )
            embedding_model = os.getenv(
                "EMBEDDING_MODEL", "Alibaba-NLP/gte-Qwen2-1.5B-instruct"
            )

            logger.info(
                f"ðŸ” [VECTORIZATION] Generating embedding | document_id={document_id} | "
                f"content_length={len(full_text)} | model={embedding_model}",
                extra={
                    "document_id": document_id,
                    "project_id": project_id,
                    "pipeline_step": "embedding_generation",
                },
            )

            # Use shared HTTP client for embedding generation
            # vLLM uses OpenAI-compatible endpoint: POST /v1/embeddings
            embedding_payload = {
                "model": embedding_model,
                "input": full_text,  # OpenAI API uses "input" not "prompt"
            }

            if shared_http_client:
                embedding_response = await shared_http_client.post(
                    f"{embedding_model_url}/v1/embeddings",
                    json=embedding_payload,
                    timeout=30.0,
                )
            else:
                async with httpx.AsyncClient(timeout=30.0) as client:
                    embedding_response = await client.post(
                        f"{embedding_model_url}/v1/embeddings",
                        json=embedding_payload,
                    )

            embedding_response.raise_for_status()
            embedding_data = embedding_response.json()

            # vLLM/OpenAI format: {"data": [{"embedding": [...]}]}
            if "data" in embedding_data and len(embedding_data["data"]) > 0:
                embedding = embedding_data["data"][0].get("embedding")
            else:
                # Fallback for other formats
                embedding = embedding_data.get("embedding")

            if not embedding:
                raise ValueError(
                    f"Embedding response missing 'embedding' field. Response keys: {list(embedding_data.keys())}"
                )

            embeddings_generated = True
            logger.info(
                f"âœ… [VECTORIZATION] Embedding generated | document_id={document_id} | "
                f"embedding_dimensions={len(embedding)}",
                extra={
                    "document_id": document_id,
                    "embedding_dimensions": len(embedding),
                    "pipeline_step": "embedding_generation",
                    "status": "success",
                },
            )

            # Index in Qdrant using QdrantIndexerEffect
            from src.effects.qdrant_indexer_effect import QdrantIndexerEffect

            logger.info(
                f"ðŸ“Š [VECTORIZATION] Indexing in Qdrant | document_id={document_id} | "
                f"source_path={source_path}",
                extra={
                    "document_id": document_id,
                    "project_id": project_id,
                    "pipeline_step": "qdrant_indexing",
                },
            )

            # Prepare file info for Qdrant
            file_info = {
                "absolute_path": source_path,
                "project_name": project_id,
                "content_hash": hashlib.blake2b(
                    full_text.encode(), digest_size=32
                ).hexdigest(),
                "metadata": metadata,
            }

            # Initialize QdrantIndexerEffect
            qdrant_url = os.getenv("QDRANT_URL", "http://qdrant:6333")
            qdrant_effect = QdrantIndexerEffect(qdrant_url=qdrant_url)

            # Execute indexing
            qdrant_result = await qdrant_effect.execute(
                {
                    "file_info": file_info,
                    "embedding": embedding,
                    "collection_name": "archon_vectors",
                    "project_name": project_id,
                }
            )

            if qdrant_result.success and qdrant_result.items_processed > 0:
                vector_indexed = True
                logger.info(
                    f"âœ… [VECTORIZATION] Qdrant indexing complete | document_id={document_id} | "
                    f"items_processed={qdrant_result.items_processed} | "
                    f"duration_ms={qdrant_result.duration_ms:.2f}",
                    extra={
                        "document_id": document_id,
                        "items_processed": qdrant_result.items_processed,
                        "duration_ms": qdrant_result.duration_ms,
                        "pipeline_step": "qdrant_indexing",
                        "status": "success",
                    },
                )
            else:
                logger.warning(
                    f"âš ï¸ [VECTORIZATION] Qdrant indexing returned no items | document_id={document_id} | "
                    f"errors={qdrant_result.errors}",
                    extra={
                        "document_id": document_id,
                        "errors": qdrant_result.errors,
                        "pipeline_step": "qdrant_indexing",
                        "status": "warning",
                    },
                )

            # Cleanup Qdrant client
            await qdrant_effect.cleanup()

        except Exception as e:
            # Log vectorization failure but don't fail entire pipeline
            logger.error(
                f"âŒ [VECTORIZATION] Vectorization failed | document_id={document_id} | "
                f"embeddings_generated={embeddings_generated} | error={str(e)}",
                extra={
                    "document_id": document_id,
                    "embeddings_generated": embeddings_generated,
                    "pipeline_step": "vectorization",
                    "status": "failed",
                    "error": str(e),
                },
                exc_info=True,
            )
            # Don't raise - allow pipeline to continue
            # Vectorization is important but not critical for entity extraction

        # Pipeline Stage Logging
        logger.info(
            f"[PIPELINE] Stage 4: Running freshness analysis | document_id={document_id} | source_path={source_path}",
            extra={"document_id": document_id, "pipeline_stage": 4, "stage_name": "freshness_analysis"}
        )

        # Step 3: Trigger freshness analysis for the new document
        # NOTE: Freshness analysis only works for filesystem paths, not archon:// URIs
        # Skip for archon:// URIs to prevent "Document not found" errors
        if freshness_event_coordinator:
            # Check if this is an archon:// URI (not a filesystem path)
            if source_path.startswith("archon://"):
                logger.info(
                    f"â­ï¸ [PIPELINE] Skipping freshness analysis for archon:// URI | document_id={document_id} | "
                    f"source_path={source_path}",
                    extra={
                        "document_id": document_id,
                        "pipeline_step": "freshness_analysis",
                        "status": "skipped",
                        "reason": "archon_uri_not_filesystem_path",
                    },
                )
                # TODO: Implement freshness analysis support for archon:// URIs
                # This requires modifying the FreshnessEventCoordinator to accept and pass
                # document content to the monitor, rather than relying on filesystem access
            else:
                # Only trigger freshness for filesystem paths
                event = DocumentUpdateEvent(
                    event_type=DocumentUpdateType.CREATED,
                    document_path=source_path,
                    priority=EVENT_PRIORITY_HIGH,
                    requires_immediate_analysis=False,  # Use batch processing
                    created_by="document_indexing_pipeline",
                )
                await freshness_event_coordinator.handle_document_update_event(event)
                logger.info(
                    f"âœ… [PIPELINE] Triggered freshness analysis | document_id={document_id}",
                    extra={
                        "document_id": document_id,
                        "pipeline_step": "freshness_analysis",
                        "status": "triggered",
                    },
                )

        # Pipeline Stage Logging: Final completion
        logger.info(
            f"[PIPELINE] Stage 5: Pipeline complete | document_id={document_id} | "
            f"embeddings_generated={embeddings_generated} | vector_indexed={vector_indexed}",
            extra={"document_id": document_id, "pipeline_stage": 5, "stage_name": "completion"}
        )

        # Log successful document processing completion
        vectorization_result = {
            "vectorization_attempted": True,
            "vectorization_status": "completed" if vector_indexed else "failed",
            "embeddings_generated": embeddings_generated,
            "vector_indexed": vector_indexed,
            "project_id": project_id,
            "source_path": source_path,
        }
        intelligence_logger.log_document_processing_complete(
            document_id=document_id,
            entities_extracted=len(entities),
            vectorization_result=vectorization_result,
        )

        logger.info(
            f"âœ… [INDEXING PIPELINE] Document processing complete | document_id={document_id} | "
            f"embeddings_generated={embeddings_generated} | vector_indexed={vector_indexed}",
            extra={
                "document_id": document_id,
                "project_id": project_id,
                "entities_count": len(entities),
                "embeddings_generated": embeddings_generated,
                "vector_indexed": vector_indexed,
                "pipeline_status": "complete",
            },
        )

        # Mark background task as complete
        if tracker:
            await tracker.complete_task(
                document_id=document_id,
                entities_extracted=len(entities),
                vector_indexed=vector_indexed,
            )

    except Exception as e:
        # Mark background task as failed
        if tracker:
            await tracker.fail_task(
                document_id=document_id,
                error=str(e),
                details={"exception_type": type(e).__name__, "project_id": project_id, "source_path": source_path},
            )
        # Re-raise for retry logic
        raise


# Batch indexing endpoint


class BatchIndexRequest(BaseModel):
    """Request model for batch document indexing"""

    documents: List[dict] = Field(
        ..., description="List of documents to index with title, content, url, etc."
    )
    batch_size: int = Field(
        default=BATCH_INDEX_DEFAULT_SIZE,
        description="Number of documents to process in parallel batches",
    )
    quality_scoring: bool = Field(
        default=True, description="Enable quality scoring for documents"
    )


class BatchIndexResult(BaseModel):
    """Result model for batch indexing operation"""

    success: bool
    total_documents: int
    processed_documents: int
    failed_documents: int
    batch_size: int
    processing_time_seconds: float
    documents_per_second: float
    failed_document_ids: List[str] = []
    error_summary: List[dict] = []


@app.post("/batch-index", response_model=BatchIndexResult)
async def batch_index_documents(
    request: BatchIndexRequest, background_tasks: BackgroundTasks
) -> BatchIndexResult:
    """
    Batch index multiple documents into the knowledge graph and vector store.

    This endpoint accepts a list of documents and processes them in batches,
    extracting entities, storing in Memgraph, and triggering vectorization
    in the search service.

    Args:
        request: BatchIndexRequest with documents list and batch configuration

    Returns:
        BatchIndexResult with processing statistics and any errors

    Example:
        POST /batch-index
        {
            "documents": [
                {
                    "title": "API Documentation",
                    "content": "Full documentation text...",
                    "url": "https://example.com/docs/api",
                    "source_domain": "example.com",
                    "metadata": {"type": "documentation", "version": "1.0"}
                }
            ],
            "batch_size": 10,
            "quality_scoring": true
        }
    """
    start_time = time.time()
    processed_count = 0
    failed_count = 0
    failed_doc_ids = []
    error_summary = []

    try:
        if not intelligence_service:
            raise HTTPException(
                status_code=503, detail="Intelligence service not initialized"
            )

        total_docs = len(request.documents)
        logger.info(
            f"ðŸ”¬ [BATCH INDEXING] Starting batch indexing | total_documents={total_docs} | "
            f"batch_size={request.batch_size} | quality_scoring={request.quality_scoring}"
        )

        # Process documents in batches
        for batch_idx in range(0, total_docs, request.batch_size):
            batch = request.documents[batch_idx : batch_idx + request.batch_size]
            batch_num = (batch_idx // request.batch_size) + 1
            total_batches = (total_docs + request.batch_size - 1) // request.batch_size

            logger.info(
                f"ðŸ“¦ [BATCH INDEXING] Processing batch {batch_num}/{total_batches} | "
                f"batch_size={len(batch)}"
            )

            # Process each document in the batch
            for doc_idx, doc in enumerate(batch):
                try:
                    # Extract document fields
                    title = doc.get("title", "Untitled Document")
                    content = doc.get("content", "")
                    url = doc.get("url", "")
                    source_domain = doc.get("source_domain", "unknown")
                    metadata = doc.get("metadata", {})

                    # Generate unique document ID using helper function
                    document_id = _generate_document_id(
                        url=url, title=title, content=content
                    )

                    # Combine title and content
                    full_text = f"{title}\n\n{content}".strip()

                    # Use URL as source path, or create synthetic one
                    source_path = (
                        url or f"batch://indexed/{source_domain}/{document_id}"
                    )

                    # Enhanced metadata
                    enhanced_metadata = _enhance_document_metadata(
                        base_metadata=metadata,
                        document_id=document_id,
                        title=title,
                        source="batch_indexing",
                        source_domain=source_domain,
                        quality_scoring_enabled=request.quality_scoring,
                        timestamp_field="batch_indexed_at",
                    )

                    logger.info(
                        f"ðŸ“„ [BATCH INDEXING] Processing document {batch_idx + doc_idx + 1}/{total_docs} | "
                        f"document_id={document_id} | title={title[:50]}... | content_length={len(full_text)}"
                    )

                    # Extract entities and relationships from document
                    entities, relationships_dict = (
                        await intelligence_service.extract_entities_from_document(
                            content=full_text,
                            source_path=source_path,
                            metadata=enhanced_metadata,
                        )
                    )

                    # Convert relationships to model objects
                    relationships = _convert_relationships_to_models(relationships_dict)

                    logger.info(
                        f"âœ… [BATCH INDEXING] Entities and relationships extracted | document_id={document_id} | "
                        f"entities_count={len(entities)} | "
                        f"relationships_count={len(relationships)}"
                    )

                    # Queue background processing for entity, relationship storage, and vectorization
                    background_tasks.add_task(
                        _process_document_background,
                        entities,
                        source_path,
                        full_text,
                        enhanced_metadata,
                        document_id,
                        source_domain,  # Using source_domain as pseudo project_id
                        relationships,  # Pass relationships to background task
                    )

                    processed_count += 1

                except Exception as doc_error:
                    failed_count += 1
                    error_doc_id = doc.get("title", f"doc_{batch_idx + doc_idx}")
                    failed_doc_ids.append(error_doc_id)
                    error_summary.append(
                        {
                            "document_id": error_doc_id,
                            "error": str(doc_error),
                            "batch": batch_num,
                            "doc_index": doc_idx,
                        }
                    )
                    logger.error(
                        f"âŒ [BATCH INDEXING] Failed to process document | "
                        f"document_id={error_doc_id} | error={doc_error}"
                    )

            # Small delay between batches to avoid overwhelming services
            if batch_idx + request.batch_size < total_docs:
                await asyncio.sleep(BATCH_PROCESSING_DELAY_SECONDS)

        # Calculate statistics
        processing_time = time.time() - start_time
        docs_per_second = (
            processed_count / processing_time if processing_time > 0 else 0
        )

        logger.info(
            f"âœ… [BATCH INDEXING] Batch indexing complete | "
            f"total={total_docs} | processed={processed_count} | failed={failed_count} | "
            f"time={processing_time:.2f}s | rate={docs_per_second:.2f} docs/sec"
        )

        return BatchIndexResult(
            success=failed_count == 0,
            total_documents=total_docs,
            processed_documents=processed_count,
            failed_documents=failed_count,
            batch_size=request.batch_size,
            processing_time_seconds=round(processing_time, 2),
            documents_per_second=round(docs_per_second, 2),
            failed_document_ids=failed_doc_ids,
            error_summary=error_summary,
        )

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"âŒ [BATCH INDEXING] Batch indexing failed catastrophically: {e}")
        processing_time = time.time() - start_time
        return BatchIndexResult(
            success=False,
            total_documents=len(request.documents),
            processed_documents=processed_count,
            failed_documents=failed_count,
            batch_size=request.batch_size,
            processing_time_seconds=round(processing_time, 2),
            documents_per_second=0.0,
            failed_document_ids=failed_doc_ids,
            error_summary=[{"error": str(e), "type": "catastrophic_failure"}],
        )


@app.post("/api/intelligence/index-directory-hierarchy")
async def index_directory_hierarchy(
    project_name: str, project_root: str, file_paths: List[str]
):
    """
    Index directory hierarchy for a project.

    Creates hierarchical structure in Memgraph:
    - PROJECT node at root
    - DIRECTORY nodes for all directories
    - CONTAINS relationships forming the hierarchy

    This is typically called after bulk ingestion to build
    the complete project structure for visualization.

    Args:
        project_name: Project identifier (e.g., "omniarchon")
        project_root: Absolute path to project root
        file_paths: List of file paths to extract directory structure from

    Returns:
        Dictionary with status and statistics

    Example:
        POST /api/intelligence/index-directory-hierarchy
        {
            "project_name": "omniarchon",
            "project_root": "/Volumes/PRO-G40/Code/omniarchon",
            "file_paths": [
                "/Volumes/PRO-G40/Code/omniarchon/services/intelligence/app.py",
                "/Volumes/PRO-G40/Code/omniarchon/services/bridge/main.py"
            ]
        }
    """
    try:
        # Validate project_name at entry point
        if not project_name or not project_name.strip():
            raise HTTPException(
                status_code=400,
                detail="project_name is required and cannot be empty. All ingested files must belong to a project.",
            )

        if not directory_indexer:
            raise HTTPException(
                status_code=503, detail="Directory indexer service not initialized"
            )

        logger.info(
            f"ðŸŒ³ [API] Directory hierarchy indexing requested | "
            f"project={project_name} | "
            f"root={project_root} | "
            f"files={len(file_paths)}"
        )

        # Index the directory hierarchy
        stats = await directory_indexer.index_directory_hierarchy(
            project_name=project_name, project_root=project_root, file_paths=file_paths
        )

        logger.info(
            f"âœ… [API] Directory hierarchy indexed successfully | "
            f"project={project_name} | "
            f"stats={stats}"
        )

        return {
            "status": "success",
            "project": project_name,
            "statistics": stats,
            "message": f"Indexed {stats['directories']} directories and {stats['files']} files",
        }

    except HTTPException:
        raise
    except Exception as e:
        logger.error(
            f"âŒ [API] Directory hierarchy indexing failed | "
            f"project={project_name} | "
            f"error={str(e)}",
            exc_info=True,
        )
        raise HTTPException(
            status_code=500, detail=f"Directory hierarchy indexing failed: {str(e)}"
        )


@app.get(
    "/api/intelligence/tree/visualize/{project_name}", response_model=TreeVisualization
)
async def visualize_tree(
    project_name: str, include_dependencies: bool = True, max_depth: int = 10
):
    """
    Generate interactive file tree visualization with dependency overlay.

    Returns hierarchical tree structure with optional import dependency arrows.
    Uses the PROJECT â†’ DIRECTORY â†’ FILE graph structure built by DirectoryIndexer.

    Args:
        project_name: Project identifier (e.g., "omniarchon")
        include_dependencies: Include IMPORTS relationships (default: True)
        max_depth: Maximum tree depth to traverse (default: 10)

    Returns:
        TreeVisualization with hierarchical tree, dependencies, and statistics

    Example:
        GET /api/intelligence/tree/visualize/omniarchon?include_dependencies=true&max_depth=10

    Response:
        {
            "tree": {
                "id": "project:omniarchon",
                "name": "omniarchon",
                "type": "project",
                "path": "/Volumes/PRO-G40/Code/omniarchon",
                "children": [...]
            },
            "dependencies": [
                {
                    "source": "file:omniarchon:services/intelligence/app.py",
                    "target": "file:omniarchon:storage/memgraph_adapter.py",
                    "type": "imports",
                    "import_type": "direct"
                }
            ],
            "statistics": {
                "total_nodes": 150,
                "projects": 1,
                "directories": 25,
                "files": 124,
                "dependencies": 89,
                "max_depth": 6
            }
        }
    """
    try:
        if not memgraph_adapter:
            raise HTTPException(
                status_code=503, detail="Memgraph adapter not initialized"
            )

        logger.info(
            f"ðŸŒ³ [TREE VIZ] Tree visualization requested | "
            f"project={project_name} | "
            f"include_dependencies={include_dependencies} | "
            f"max_depth={max_depth}"
        )

        # Build hierarchical tree structure
        tree = await _build_tree_structure(project_name, max_depth)

        if not tree:
            raise HTTPException(
                status_code=404, detail=f"Project not found: {project_name}"
            )

        # Extract dependencies if requested
        dependencies = []
        if include_dependencies:
            dependencies = await _extract_dependencies(project_name)

        # Gather statistics
        statistics = await _gather_tree_statistics(project_name, tree, dependencies)

        logger.info(
            f"âœ… [TREE VIZ] Tree visualization generated | "
            f"project={project_name} | "
            f"nodes={statistics.total_nodes} | "
            f"dependencies={statistics.dependencies}"
        )

        return TreeVisualization(
            tree=tree,
            dependencies=dependencies,
            statistics=statistics,
            project_name=project_name,
        )

    except HTTPException:
        raise
    except Exception as e:
        logger.error(
            f"âŒ [TREE VIZ] Tree visualization failed | "
            f"project={project_name} | "
            f"error={str(e)}",
            exc_info=True,
        )
        raise HTTPException(
            status_code=500, detail=f"Tree visualization failed: {str(e)}"
        )


# ============================================================================
# ORPHAN FILE DETECTION APIs
# ============================================================================


@app.get("/api/intelligence/orphans/detect/{project_name}")
async def detect_orphans(project_name: str, entry_points: Optional[str] = None):
    """
    Detect orphaned and unreachable files in a project.

    Identifies three types of orphaned files:
    1. **No Incoming Imports**: Files that no other file imports
    2. **Unreachable from Entry Points**: Files not in dependency chain from main files
    3. **Dead Code**: Orphaned files that also define no used entities

    Args:
        project_name: Project to analyze (e.g., "omniarchon")
        entry_points: Optional custom entry points (comma-separated, e.g., "main.py,app.py")

    Returns:
        OrphanDetectionResult with categorized orphaned files

    Example:
        GET /api/intelligence/orphans/detect/omniarchon
        GET /api/intelligence/orphans/detect/omniarchon?entry_points=main.py,server.py

    Response:
        {
            "project": "omniarchon",
            "orphaned_files": [...],
            "unreachable_files": [...],
            "dead_code_files": [...],
            "total_files": 245,
            "total_orphans": 12,
            "entry_points": ["services/intelligence/app.py"],
            "scan_timestamp": "2025-11-07T15:30:00Z"
        }
    """
    try:
        if not orphan_detector:
            raise HTTPException(
                status_code=503, detail="Orphan detector not initialized"
            )

        # Parse custom entry points if provided
        custom_entry_points = entry_points.split(",") if entry_points else None

        logger.info(
            f"ðŸ” [ORPHAN API] Orphan detection requested | "
            f"project={project_name} | "
            f"custom_entry_points={custom_entry_points}"
        )

        result = await orphan_detector.detect_orphans(
            project_name=project_name, custom_entry_points=custom_entry_points
        )

        logger.info(
            f"âœ… [ORPHAN API] Orphan detection complete | "
            f"project={project_name} | "
            f"total_orphans={result.total_orphans}"
        )

        return result

    except HTTPException:
        raise
    except Exception as e:
        logger.error(
            f"âŒ [ORPHAN API] Orphan detection failed | "
            f"project={project_name} | "
            f"error={str(e)}",
            exc_info=True,
        )
        raise HTTPException(
            status_code=500, detail=f"Orphan detection failed: {str(e)}"
        )


@app.get("/api/intelligence/orphans/summary/{project_name}")
async def get_orphan_summary(project_name: str):
    """
    Get quick summary of orphaned files without full file lists.

    Returns statistics and percentages for quick analysis.

    Args:
        project_name: Project to analyze

    Returns:
        Summary statistics including orphan percentages

    Example:
        GET /api/intelligence/orphans/summary/omniarchon

    Response:
        {
            "project": "omniarchon",
            "total_files": 245,
            "orphan_count": 12,
            "unreachable_count": 8,
            "dead_code_count": 3,
            "orphan_percentage": 4.9,
            "entry_points": ["services/intelligence/app.py"],
            "scan_timestamp": "2025-11-07T15:30:00Z"
        }
    """
    try:
        if not orphan_detector:
            raise HTTPException(
                status_code=503, detail="Orphan detector not initialized"
            )

        logger.info(
            f"ðŸ” [ORPHAN API] Orphan summary requested | " f"project={project_name}"
        )

        result = await orphan_detector.detect_orphans(project_name=project_name)

        summary = {
            "project": project_name,
            "total_files": result.total_files,
            "orphan_count": result.total_orphans,
            "unreachable_count": len(result.unreachable_files),
            "dead_code_count": len(result.dead_code_files),
            "orphan_percentage": round(
                (
                    (result.total_orphans / result.total_files * 100)
                    if result.total_files > 0
                    else 0
                ),
                2,
            ),
            "entry_points": result.entry_points,
            "scan_timestamp": result.scan_timestamp,
        }

        logger.info(
            f"âœ… [ORPHAN API] Orphan summary complete | "
            f"project={project_name} | "
            f"orphan_percentage={summary['orphan_percentage']}%"
        )

        return summary

    except HTTPException:
        raise
    except Exception as e:
        logger.error(
            f"âŒ [ORPHAN API] Orphan summary failed | "
            f"project={project_name} | "
            f"error={str(e)}",
            exc_info=True,
        )
        raise HTTPException(status_code=500, detail=f"Orphan summary failed: {str(e)}")


@app.get("/api/intelligence/orphans/dead-code/{project_name}")
async def get_dead_code_files(project_name: str):
    """
    Get only dead code files (highest confidence deletion candidates).

    Dead code files are orphaned AND have no used entities, making them
    the safest candidates for removal.

    Args:
        project_name: Project to analyze

    Returns:
        List of dead code files with metadata

    Example:
        GET /api/intelligence/orphans/dead-code/omniarchon

    Response:
        {
            "project": "omniarchon",
            "dead_code_files": [
                {
                    "file_path": "/Volumes/.../unused.py",
                    "relative_path": "services/unused.py",
                    "orphan_type": "dead_code",
                    "reason": "File has no imports and defines no used entities",
                    "import_count": 0,
                    "entity_count": 2,
                    "last_modified": "2024-08-20T14:00:00Z"
                }
            ],
            "count": 1,
            "scan_timestamp": "2025-11-07T15:30:00Z"
        }
    """
    try:
        if not orphan_detector:
            raise HTTPException(
                status_code=503, detail="Orphan detector not initialized"
            )

        logger.info(
            f"ðŸ” [ORPHAN API] Dead code files requested | " f"project={project_name}"
        )

        result = await orphan_detector.detect_orphans(project_name=project_name)

        response = {
            "project": project_name,
            "dead_code_files": result.dead_code_files,
            "count": len(result.dead_code_files),
            "scan_timestamp": result.scan_timestamp,
        }

        logger.info(
            f"âœ… [ORPHAN API] Dead code files retrieved | "
            f"project={project_name} | "
            f"count={response['count']}"
        )

        return response

    except HTTPException:
        raise
    except Exception as e:
        logger.error(
            f"âŒ [ORPHAN API] Dead code retrieval failed | "
            f"project={project_name} | "
            f"error={str(e)}",
            exc_info=True,
        )
        raise HTTPException(
            status_code=500, detail=f"Dead code retrieval failed: {str(e)}"
        )


# ============================================================================
# TREE STRUCTURE HELPER FUNCTIONS
# ============================================================================


async def _build_tree_structure(
    project_name: str, max_depth: int
) -> Optional[TreeNode]:
    """
    Build hierarchical tree structure from Memgraph graph.

    Queries PROJECT â†’ DIRECTORY â†’ FILE hierarchy using variable-length
    CONTAINS paths with depth limit.

    Args:
        project_name: Project identifier
        max_depth: Maximum traversal depth

    Returns:
        Root TreeNode with recursive children, or None if project not found
    """
    query = """
    MATCH path = (project:PROJECT {name: $project_name})-[:CONTAINS*0..1]->(child)
    RETURN
        project.entity_id AS entity_id,
        project.name AS name,
        project.path AS path,
        'project' AS node_type,
        null AS metadata,
        collect(DISTINCT {
            entity_id: child.entity_id,
            name: child.name,
            path: child.path,
            type: labels(child)[0]
        }) AS direct_children
    """

    try:
        async with memgraph_adapter.driver.session() as session:
            result = await session.run(query, project_name=project_name)
            record = await result.single()

            if not record:
                logger.warning(f"Project not found: {project_name}")
                return None

            # Build root project node
            root_node = TreeNode(
                id=record["entity_id"],
                name=record["name"],
                type="project",
                path=record["path"] or "",
                children=[],
                metadata=None,
            )

            # Build children recursively
            direct_children = record["direct_children"]
            for child_data in direct_children:
                if child_data["entity_id"]:  # Skip null children
                    child_node = await _build_node_recursively(
                        child_data["entity_id"],
                        child_data["type"].lower(),
                        max_depth - 1,
                    )
                    if child_node:
                        root_node.children.append(child_node)

            logger.debug(
                f"ðŸŒ³ [TREE BUILD] Root node built | "
                f"project={project_name} | "
                f"direct_children={len(root_node.children)}"
            )

            return root_node

    except Exception as e:
        logger.error(
            f"âŒ [TREE BUILD] Failed to build tree structure | "
            f"project={project_name} | "
            f"error={str(e)}",
            exc_info=True,
        )
        raise


async def _build_node_recursively(
    entity_id: str, node_type: str, remaining_depth: int
) -> Optional[TreeNode]:
    """
    Recursively build tree node with children.

    Args:
        entity_id: Node entity_id
        node_type: Node type (directory/file)
        remaining_depth: Remaining depth to traverse

    Returns:
        TreeNode with children, or None if depth exceeded
    """
    if remaining_depth < 0:
        return None

    query = """
    MATCH (node {entity_id: $entity_id})
    OPTIONAL MATCH (node)-[:CONTAINS]->(child)
    RETURN
        node.entity_id AS entity_id,
        node.name AS name,
        node.path AS path,
        labels(node)[0] AS node_label,
        node.file_size AS file_size,
        node.language AS language,
        node.entity_count AS entity_count,
        node.line_count AS line_count,
        node.last_modified AS last_modified,
        node.indexed_at AS indexed_at,
        collect(DISTINCT {
            entity_id: child.entity_id,
            type: labels(child)[0]
        }) AS children_data
    """

    try:
        async with memgraph_adapter.driver.session() as session:
            result = await session.run(query, entity_id=entity_id)
            record = await result.single()

            if not record:
                return None

            # Build metadata for files
            metadata = None
            if node_type == "file":
                metadata = TreeNodeMetadata(
                    file_size=record.get("file_size"),
                    language=record.get("language"),
                    entity_count=record.get("entity_count"),
                    line_count=record.get("line_count"),
                    last_modified=record.get("last_modified"),
                    indexed_at=record.get("indexed_at"),
                )

            # Create current node
            current_node = TreeNode(
                id=record["entity_id"],
                name=record["name"],
                type=node_type,
                path=record["path"] or "",
                children=[],
                metadata=metadata,
            )

            # Recursively build children if depth allows
            if remaining_depth > 0:
                children_data = record["children_data"]
                for child_data in children_data:
                    if child_data["entity_id"]:  # Skip null children
                        child_type = (
                            child_data["type"].lower()
                            if child_data["type"]
                            else "unknown"
                        )
                        child_node = await _build_node_recursively(
                            child_data["entity_id"], child_type, remaining_depth - 1
                        )
                        if child_node:
                            current_node.children.append(child_node)

            return current_node

    except Exception as e:
        logger.error(
            f"âŒ [TREE BUILD] Failed to build node recursively | "
            f"entity_id={entity_id} | "
            f"error={str(e)}",
            exc_info=True,
        )
        return None


async def _extract_dependencies(project_name: str) -> List[DependencyEdge]:
    """
    Extract import dependencies from IMPORTS relationships.

    Queries all IMPORTS relationships between FILE nodes in the project.

    Args:
        project_name: Project identifier

    Returns:
        List of DependencyEdge objects representing import relationships
    """
    query = """
    MATCH (source:FILE)-[r:IMPORTS]->(target:FILE)
    WHERE source.project_name = $project_name
    RETURN
        source.entity_id AS source_id,
        target.entity_id AS target_id,
        r.import_type AS import_type,
        r.confidence AS confidence
    """

    dependencies = []

    try:
        async with memgraph_adapter.driver.session() as session:
            result = await session.run(query, project_name=project_name)

            async for record in result:
                dependency = DependencyEdge(
                    source=record["source_id"],
                    target=record["target_id"],
                    type="imports",
                    import_type=record.get("import_type"),
                    confidence=record.get("confidence"),
                )
                dependencies.append(dependency)

        logger.debug(
            f"ðŸŒ³ [DEPENDENCIES] Extracted {len(dependencies)} dependencies | "
            f"project={project_name}"
        )

        return dependencies

    except Exception as e:
        logger.error(
            f"âŒ [DEPENDENCIES] Failed to extract dependencies | "
            f"project={project_name} | "
            f"error={str(e)}",
            exc_info=True,
        )
        return []


async def _gather_tree_statistics(
    project_name: str, tree: TreeNode, dependencies: List[DependencyEdge]
) -> TreeStatistics:
    """
    Gather tree visualization statistics.

    Counts nodes by type and calculates tree metrics.

    Args:
        project_name: Project identifier
        tree: Root tree node
        dependencies: List of dependency edges

    Returns:
        TreeStatistics with counts and metrics
    """
    query = """
    MATCH (project:PROJECT {name: $project_name})
    OPTIONAL MATCH (project)-[:CONTAINS*]->(dir:DIRECTORY)
    OPTIONAL MATCH (project)-[:CONTAINS*]->(file:FILE)
    RETURN
        count(DISTINCT project) AS project_count,
        count(DISTINCT dir) AS directory_count,
        count(DISTINCT file) AS file_count
    """

    try:
        async with memgraph_adapter.driver.session() as session:
            result = await session.run(query, project_name=project_name)
            record = await result.single()

            if record:
                projects = record["project_count"]
                directories = record["directory_count"]
                files = record["file_count"]
            else:
                projects = 1 if tree else 0
                directories = 0
                files = 0

        # Calculate max depth by traversing tree
        max_depth = _calculate_tree_depth(tree)

        # Calculate total nodes
        total_nodes = projects + directories + files

        statistics = TreeStatistics(
            total_nodes=total_nodes,
            projects=projects,
            directories=directories,
            files=files,
            dependencies=len(dependencies),
            max_depth=max_depth,
        )

        logger.debug(
            f"ðŸŒ³ [STATISTICS] Gathered tree statistics | "
            f"project={project_name} | "
            f"total_nodes={total_nodes} | "
            f"max_depth={max_depth}"
        )

        return statistics

    except Exception as e:
        logger.error(
            f"âŒ [STATISTICS] Failed to gather statistics | "
            f"project={project_name} | "
            f"error={str(e)}",
            exc_info=True,
        )
        return TreeStatistics(
            total_nodes=0,
            projects=0,
            directories=0,
            files=0,
            dependencies=len(dependencies),
            max_depth=0,
        )


def _calculate_tree_depth(node: Optional[TreeNode], current_depth: int = 0) -> int:
    """
    Calculate maximum depth of tree by recursive traversal.

    Args:
        node: Current tree node
        current_depth: Current depth level

    Returns:
        Maximum depth from this node
    """
    if not node or not node.children:
        return current_depth

    max_child_depth = current_depth
    for child in node.children:
        child_depth = _calculate_tree_depth(child, current_depth + 1)
        max_child_depth = max(max_child_depth, child_depth)

    return max_child_depth


# Event-driven freshness system endpoints


@app.post("/freshness/events/document-update")
async def trigger_document_update_event(
    document_path: str,
    event_type: str = "UPDATED",
    priority: int = EVENT_PRIORITY_NORMAL,
    immediate: bool = False,
):
    """Manually trigger a document update event for testing"""
    try:
        if not freshness_event_coordinator:
            raise HTTPException(
                status_code=503, detail="Event coordinator not initialized"
            )

        # Create document update event
        event = DocumentUpdateEvent(
            event_type=DocumentUpdateType(event_type),
            document_path=document_path,
            priority=priority,
            requires_immediate_analysis=immediate,
            created_by="manual_api_trigger",
        )

        # Process the event
        await freshness_event_coordinator.handle_document_update_event(event)

        return {
            "event_id": str(event.event_id),
            "event_type": event.event_type,
            "document_path": event.document_path,
            "processed_immediately": immediate,
            "status": "event_processed",
        }

    except ValueError as e:
        raise HTTPException(status_code=400, detail=f"Invalid event type: {e}")
    except Exception as e:
        logger.error(f"Failed to process document update event: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@app.get("/freshness/events/stats")
async def get_event_coordinator_stats():
    """Get performance statistics from the event coordinator"""
    try:
        if not freshness_event_coordinator:
            raise HTTPException(
                status_code=503, detail="Event coordinator not initialized"
            )

        stats = await freshness_event_coordinator.get_stats()

        return {"coordinator_stats": stats, "status": "active"}

    except Exception as e:
        logger.error(f"Failed to get event coordinator stats: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@app.get("/metrics")
async def metrics():
    """Prometheus metrics endpoint"""
    return Response(generate_latest(), media_type=CONTENT_TYPE_LATEST)


# MVP Day 3: Kafka Consumer Health and Metrics Endpoints


@app.get("/kafka/health")
async def kafka_consumer_health():
    """
    Get Kafka consumer health status.

    Returns detailed health information including:
    - Consumer running status
    - Event processing metrics
    - Error rates and failure analysis
    - Recent activity status
    - Prometheus metrics integration status
    """
    if not kafka_consumer:
        return {
            "status": "disabled",
            "message": "Kafka consumer is not initialized",
            "kafka_enabled": os.getenv("KAFKA_ENABLE_CONSUMER", "true").lower()
            == "true",
        }

    try:
        health = kafka_consumer.get_health()
        metrics = kafka_consumer.get_metrics()

        # Calculate failure rate
        total_events = metrics.get("total_events", 0)
        events_failed = metrics.get("events_failed", 0)
        failure_rate = (events_failed / total_events * 100) if total_events > 0 else 0.0

        # Enhance health data with failure metrics
        health["failure_metrics"] = {
            "failure_rate_percent": round(failure_rate, 2),
            "total_failures": events_failed,
            "total_events": total_events,
            "dlq_routed": metrics.get("dlq_routed", 0),
        }

        # Add alerting status
        health["alerting"] = {
            "prometheus_metrics": "enabled",
            "slack_alerting": (
                "enabled"
                if os.getenv("SLACK_ALERTING_ENABLED", "true").lower() == "true"
                else "disabled"
            ),
            "webhook_configured": bool(os.getenv("SLACK_WEBHOOK_URL")),
        }

        # Determine overall health status based on failure rate
        if failure_rate > 50:
            health["status"] = "critical"
        elif failure_rate > 10:
            health["status"] = "degraded"

        return {
            "kafka_consumer": health,
            "timestamp": datetime.now(timezone.utc).isoformat(),
        }
    except Exception as e:
        logger.error(f"Failed to get Kafka consumer health: {e}")
        return {
            "status": "error",
            "error": str(e),
            "timestamp": datetime.now(timezone.utc).isoformat(),
        }


@app.get("/kafka/metrics")
async def kafka_consumer_metrics():
    """
    Get Kafka consumer metrics.

    Returns comprehensive metrics including:
    - Events processed/failed with failure rate analysis
    - Processing times and latency percentiles
    - Throughput (events per second)
    - Uptime and availability
    - Handler information
    - DLQ routing statistics
    - Backpressure metrics
    - Prometheus metrics exposure status
    """
    if not kafka_consumer:
        return {
            "status": "disabled",
            "message": "Kafka consumer is not initialized",
            "kafka_enabled": os.getenv("KAFKA_ENABLE_CONSUMER", "true").lower()
            == "true",
        }

    try:
        metrics = kafka_consumer.get_metrics()

        # Calculate additional derived metrics
        total_events = metrics.get("total_events", 0)
        events_failed = metrics.get("events_failed", 0)
        events_processed = metrics.get("events_processed", 0)

        # Failure rate analysis
        failure_rate = (events_failed / total_events * 100) if total_events > 0 else 0.0
        success_rate = (
            (events_processed / total_events * 100) if total_events > 0 else 0.0
        )

        # Add derived metrics
        metrics["derived_metrics"] = {
            "failure_rate_percent": round(failure_rate, 2),
            "success_rate_percent": round(success_rate, 2),
            "dlq_rate_percent": round(
                (
                    (metrics.get("dlq_routed", 0) / total_events * 100)
                    if total_events > 0
                    else 0.0
                ),
                2,
            ),
        }

        # Add health thresholds for alerting context
        metrics["health_thresholds"] = {
            "failure_rate_warning": 10.0,
            "failure_rate_critical": 50.0,
            "backpressure_warning_seconds": 1.0,
            "processing_latency_warning_seconds": 5.0,
        }

        # Add Prometheus metrics exposure info
        metrics["prometheus"] = {
            "metrics_endpoint": "/metrics",
            "metrics_exposed": [
                "kafka_event_processing_failures_total",
                "kafka_event_processing_total",
                "kafka_event_processing_duration_seconds",
                "kafka_dlq_routed_total",
                "kafka_consumer_in_flight_events",
                "kafka_consumer_backpressure_wait_seconds",
            ],
        }

        return {
            "kafka_consumer_metrics": metrics,
            "timestamp": datetime.now(timezone.utc).isoformat(),
        }
    except Exception as e:
        logger.error(f"Failed to get Kafka consumer metrics: {e}")
        return {
            "status": "error",
            "error": str(e),
            "timestamp": datetime.now(timezone.utc).isoformat(),
        }


@app.get("/health/tree-stamping")
async def tree_stamping_health():
    """
    Get Tree + Stamping event adapter health status.

    Returns detailed health information including:
    - Handler registration status
    - Topic subscription status
    - Last processed event timestamp
    - Processing metrics (events handled, failures)
    - Service availability (OnexTree, MetadataStamping)
    - Configuration status

    Created: 2025-10-24
    Purpose: Monitor Tree + Stamping event-driven integration health
    """
    # Optional admin guard for production
    if os.getenv("ENVIRONMENT", "development") == "production":
        required = os.getenv("ADMIN_TOKEN")
        if required:
            # Note: Full implementation would extract from request headers
            # For now, check TEST_ADMIN_TOKEN for testing purposes
            provided = os.getenv("TEST_ADMIN_TOKEN", "")
            if provided != required:
                return {
                    "status": "forbidden",
                    "message": "Admin token required in production",
                    "timestamp": datetime.now(timezone.utc).isoformat(),
                }

    try:
        # Check if Kafka consumer is initialized
        if not kafka_consumer:
            return {
                "status": "disabled",
                "message": "Kafka consumer not initialized - Tree Stamping adapter unavailable",
                "kafka_enabled": os.getenv("KAFKA_ENABLE_CONSUMER", "true").lower()
                == "true",
                "timestamp": datetime.now(timezone.utc).isoformat(),
            }

        # Find TreeStampingHandler in registered handlers
        tree_stamping_handler = None
        for handler in kafka_consumer.handlers:
            handler_name = getattr(
                handler, "get_handler_name", lambda: type(handler).__name__
            )()
            if "TreeStamping" in handler_name:
                tree_stamping_handler = handler
                break

        if not tree_stamping_handler:
            return {
                "status": "not_registered",
                "message": "TreeStampingHandler not registered in Kafka consumer",
                "registered_handlers": [
                    getattr(h, "get_handler_name", lambda: type(h).__name__)()
                    for h in kafka_consumer.handlers
                ],
                "timestamp": datetime.now(timezone.utc).isoformat(),
            }

        # Get handler metrics
        handler_metrics = getattr(tree_stamping_handler, "metrics", {})

        # Get topic subscription status
        tree_stamping_topics = [
            os.getenv(
                "KAFKA_TREE_INDEX_PROJECT_REQUEST",
                "dev.archon-intelligence.tree.index-project-requested.v1",
            ),
            os.getenv(
                "KAFKA_TREE_SEARCH_FILES_REQUEST",
                "dev.archon-intelligence.tree.search-files-requested.v1",
            ),
            os.getenv(
                "KAFKA_TREE_GET_STATUS_REQUEST",
                "dev.archon-intelligence.tree.get-status-requested.v1",
            ),
        ]

        subscribed_topics = kafka_consumer.topics or []
        topics_status = {
            topic: "subscribed" if topic in subscribed_topics else "not_subscribed"
            for topic in tree_stamping_topics
        }

        # Calculate health status
        events_handled = handler_metrics.get("events_handled", 0)
        events_failed = handler_metrics.get("events_failed", 0)
        failure_rate = (
            (events_failed / events_handled * 100) if events_handled > 0 else 0.0
        )

        # Determine overall status
        if failure_rate > 50:
            overall_status = "critical"
        elif failure_rate > 10:
            overall_status = "degraded"
        elif any(status == "not_subscribed" for status in topics_status.values()):
            overall_status = "degraded"
        else:
            overall_status = "healthy"

        return {
            "status": overall_status,
            "handler": {
                "registered": True,
                "handler_name": getattr(
                    tree_stamping_handler,
                    "get_handler_name",
                    lambda: "TreeStampingHandler",
                )(),
                "events_handled": events_handled,
                "events_failed": events_failed,
                "failure_rate_percent": round(failure_rate, 2),
                "total_processing_time_ms": handler_metrics.get(
                    "total_processing_time_ms", 0.0
                ),
                "average_processing_time_ms": (
                    handler_metrics.get("total_processing_time_ms", 0.0)
                    / events_handled
                    if events_handled > 0
                    else 0.0
                ),
            },
            "topics": {
                "subscribed_topics": topics_status,
                "all_subscribed": all(
                    status == "subscribed" for status in topics_status.values()
                ),
            },
            "configuration": {
                "batch_size": int(os.getenv("TREE_STAMPING_BATCH_SIZE", "100")),
                "parallel_enabled": os.getenv(
                    "TREE_STAMPING_ENABLE_PARALLEL", "true"
                ).lower()
                == "true",
                "max_workers": int(os.getenv("TREE_STAMPING_MAX_WORKERS", "4")),
            },
            "services": {
                "onex_tree_url": os.getenv(
                    "ONEX_TREE_SERVICE_URL", "http://archon-tree:8058"
                ),
                "metadata_stamping_url": os.getenv(
                    "METADATA_STAMPING_SERVICE_URL", "http://archon-stamping:8057"
                ),
            },
            "timestamp": datetime.now(timezone.utc).isoformat(),
        }

    except Exception as e:
        logger.error(f"Failed to get tree stamping health: {e}", exc_info=True)
        return {
            "status": "error",
            "error": str(e),
            "timestamp": datetime.now(timezone.utc).isoformat(),
        }


if __name__ == "__main__":
    import uvicorn

    port = int(os.getenv("INTELLIGENCE_SERVICE_PORT", 8053))
    uvicorn.run("app:app", host="0.0.0.0", port=port, reload=True, log_level="info")
