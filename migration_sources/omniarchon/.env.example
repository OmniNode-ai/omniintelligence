# ============================================================================
# OmniArchon Configuration Template
# ============================================================================
# This is the MASTER REFERENCE for all configuration variables.
#
# QUICK START:
# 1. Copy an environment template:
#    - Development:  cp .env.development .env
#    - Staging:      cp .env.staging .env
#    - Production:   cp .env.production .env
#
# 2. Edit .env with your specific values (passwords, API keys)
#
# 3. Start services:
#    cd deployment
#    docker compose up -d
#
# ENVIRONMENT SWITCHING:
# You can switch environments without modifying files:
#   docker compose --env-file ../.env.development up
#   docker compose --env-file ../.env.production up -d
#
# BENEFITS OF THIS SYSTEM:
# ‚úÖ Sensible defaults - Services work out-of-the-box
# ‚úÖ Environment-specific templates - Quick setup for dev/staging/prod
# ‚úÖ Fail-safe for secrets - Required passwords must be set
# ‚úÖ No hardcoded values - All configuration via environment
#
# ============================================================================
# Archon Intelligence Platform Configuration
# Configure services, databases, and intelligence components

# Optional: Set log level for debugging
LOGFIRE_TOKEN=
LOG_LEVEL=INFO

# Service Authentication Token for MCP tools
# Generate a secure random string (e.g., 32-character hex: openssl rand -hex 32)
# This token enables MCP tools to authenticate with the API server
SERVICE_AUTH_TOKEN=

# Service Ports Configuration
# These ports are used for external access to the services
HOST=localhost
ARCHON_SERVER_PORT=8181
ARCHON_AGENTS_PORT=8052
ARCHON_UI_PORT=3737
ARCHON_DOCS_PORT=3838

# Phase 5 Intelligence Services Configuration
INTELLIGENCE_SERVICE_PORT=8053
BRIDGE_SERVICE_PORT=8054
SEARCH_SERVICE_PORT=8055

# ==========================================
# INTELLIGENCE ENRICHMENT CONTROL
# ==========================================
# SKIP_INTELLIGENCE_ENRICHMENT - Intelligence Service Bypass
#
# Purpose: Allow direct document indexing without intelligence enrichment
# Use Case: Short-term fix when intelligence service times out
#
# Values:
#   - false (default): Full intelligence enrichment enabled
#   - true: Skip intelligence calls, direct Memgraph indexing
#
# Impact when true:
#   ‚úÖ Faster indexing (< 500ms vs 10s+ timeout)
#   ‚úÖ Documents reach Memgraph/Qdrant immediately
#   ‚ö†Ô∏è  No AI-generated quality scores or entity enrichment
#   ‚ö†Ô∏è  Qdrant vectors may be basic (no semantic embeddings)
#
# Recommended: Set to 'true' until async event-driven intelligence is implemented
SKIP_INTELLIGENCE_ENRICHMENT=false

# CORS Configuration (Security)
# Comma-separated list of allowed origins for CORS requests
# Development: Leave unset to use localhost defaults (http://localhost:3737, http://localhost:8181)
# Production: REQUIRED - Set to specific domains (e.g., https://archon.yourdomain.com,https://app.yourdomain.com)
# SECURITY WARNING: Wildcard "*" is NOT allowed in production and will be rejected
CORS_ALLOWED_ORIGINS=

# Qdrant Vector Database Configuration
QDRANT_PORT=6333
QDRANT_GRPC_PORT=6334
QDRANT_URL=http://qdrant:6333  # Full URL for Qdrant API (Docker: qdrant:6333, Host: localhost:6333)

# Frontend Configuration
# VITE_ALLOWED_HOSTS: Comma-separated list of additional hosts allowed for Vite dev server
# Example: VITE_ALLOWED_HOSTS=192.168.1.100,myhost.local,example.com
# If not set, defaults to localhost, 127.0.0.1, ::1, and the HOST value above
VITE_ALLOWED_HOSTS=

# When enabled, PROD mode will proxy ARCHON_SERVER_PORT through ARCHON_UI_PORT. This exposes both the
# Archon UI and API through a single port. This is useful when deploying Archon behind a reverse
# proxy where you want to expose the frontend on a single external domain.
PROD=false

# ==============================================================================
# Embedding Configuration
# ==============================================================================
# ‚ö†Ô∏è CRITICAL: EMBEDDING_DIMENSIONS must match EMBEDDING_MODEL output dimensions
# Dimension mismatches will cause vector search failures and indexing errors.

# Embedding service endpoint
# vLLM (recommended for GPU-accelerated embeddings):
#   - http://192.168.86.201:8002 (GPU machine 1 - 4090)
#   - http://192.168.86.202:8002 (GPU machine 2 - 5090)
# OpenAI API:
#   - https://api.openai.com/v1 (OpenAI cloud)
EMBEDDING_MODEL_URL=http://192.168.86.201:8002

# Model used for generating embeddings
# vLLM models (recommended):
#   - Alibaba-NLP/gte-Qwen2-1.5B-instruct (1536 dimensions, recommended for vLLM)
# OpenAI models:
#   - text-embedding-3-small (1536 dimensions, recommended)
#   - text-embedding-3-large (3072 dimensions, higher quality)
#   - text-embedding-ada-002 (1536 dimensions, legacy)
EMBEDDING_MODEL=Alibaba-NLP/gte-Qwen2-1.5B-instruct

# Dimensions for embedding vectors
# MUST match the model's output dimensions (see table above):
#   Alibaba-NLP/gte-Qwen2-1.5B-instruct: 1536 (vLLM)
#   text-embedding-3-small: 1536 (OpenAI)
#   text-embedding-3-large: 3072 (OpenAI)
#   nomic-embed-text: 768 (Ollama)
#   mxbai-embed-large: 1024 (Ollama)
# Changing this requires re-indexing all vectors in Qdrant
EMBEDDING_DIMENSIONS=1536

# Maximum concurrent embedding requests per consumer
# Prevents queue buildup at vLLM service during bulk operations
# - Default: 3 (conservative, prevents timeout errors)
# - Range: 1-10 (higher = more concurrent, may cause queuing)
# - Formula: N consumers √ó max_concurrent = total vLLM load
# - Example: 4 consumers √ó 3 concurrent = 12 total requests
# Performance impact:
#   - Unthrottled: 91ms base + 15-30s queue wait = timeout errors
#   - With limit (3): 91ms base + minimal queue = ~200ms latency
# Tuning: Increase to 5-10 if vLLM service can handle higher load
EMBEDDING_MAX_CONCURRENT=3

# Document Chunking Configuration
# Automatically chunks large documents before embedding to prevent vLLM errors
# - ENABLE_CHUNKING: Enable/disable automatic chunking (default: true)
# - CHUNK_MAX_TOKENS: Maximum tokens per chunk (default: 7500, safe for 8192 limit)
# - CHUNK_OVERLAP_TOKENS: Token overlap between chunks for context (default: 100)
# Why needed: vLLM embedding service crashes on documents >8192 tokens
# How it works:
#   1. Count tokens using tiktoken (cl100k_base encoding)
#   2. If >CHUNK_MAX_TOKENS, split using semchunk (semantic boundaries)
#   3. Generate embeddings for each chunk separately
#   4. Store in Qdrant with chunk metadata (chunk_index, total_chunks, parent_document_id)
# Metadata stored: chunk_index, total_chunks, is_chunk, parent_document_id, chunk_token_count
ENABLE_CHUNKING=true
CHUNK_MAX_TOKENS=7500
CHUNK_OVERLAP_TOKENS=100

# ==========================================
# TIMEOUT CONFIGURATION (all values in seconds)
# ==========================================
# These values configure timeouts across all services for HTTP requests,
# database operations, cache operations, and async operations.
# All timeout values have sensible defaults and validation ranges.

# HTTP Client Timeouts
HTTP_TIMEOUT_DEFAULT=30.0                    # Default HTTP client timeout (1.0-300.0)
HTTP_TIMEOUT_HEALTH_CHECK=5.0               # Health check timeout (1.0-30.0)
HTTP_TIMEOUT_INTELLIGENCE=60.0              # Intelligence service operations (10.0-300.0)
HTTP_TIMEOUT_SEARCH=45.0                    # Search service operations (10.0-300.0)
HTTP_TIMEOUT_LANGEXTRACT=90.0               # Langextract operations (10.0-300.0)
HTTP_TIMEOUT_BRIDGE=30.0                    # Bridge service operations (10.0-300.0)
HTTP_TIMEOUT_MCP=30.0                       # MCP service operations (10.0-300.0)
HTTP_TIMEOUT_OPTIMIZATION=120.0             # Optimization operations (30.0-600.0)
HTTP_TIMEOUT_BASELINE_COLLECTION=60.0       # Performance baseline collection (30.0-300.0)
HTTP_TIMEOUT_CONNECT=10.0                   # HTTP connection timeout (1.0-60.0)
HTTP_TIMEOUT_READ=30.0                      # HTTP read timeout (5.0-300.0)
HTTP_TIMEOUT_WRITE=5.0                      # HTTP write timeout (1.0-60.0)

# Intelligence Service Client Configuration
# Timeout for code analysis requests (assess_code_quality, pattern detection, performance analysis)
# Increased from 10.0s to 30.0s to accommodate complex queries and reduce timeout failures
# Performance targets:
#   - Simple queries: <500ms
#   - Complex queries: <2000ms
#   - Timeout threshold: 30000ms (30s)
INTELLIGENCE_CLIENT_TIMEOUT=30.0             # Intelligence client default timeout (10.0-120.0)

# HTTP Retry Configuration (Exponential Backoff)
# Retries on transient failures (network errors, 5xx responses)
# Does NOT retry on 4xx client errors
HTTP_CLIENT_MAX_RETRIES=3                          # Maximum retry attempts (0-10, default: 3)
HTTP_CLIENT_RETRY_BACKOFF_DELAYS=1.0,2.0,4.0      # Comma-separated delays in seconds (default: 1s,2s,4s)

# Service-specific retry overrides (optional)
SEARCH_SERVICE_HTTP_CLIENT_MAX_RETRIES=3
SEARCH_SERVICE_HTTP_CLIENT_RETRY_BACKOFF_DELAYS=1.0,2.0,4.0

BRIDGE_SERVICE_HTTP_CLIENT_MAX_RETRIES=3
BRIDGE_SERVICE_HTTP_CLIENT_RETRY_BACKOFF_DELAYS=1.0,2.0,4.0

# Database Timeouts
DB_TIMEOUT_CONNECTION=30.0                  # Database connection timeout (5.0-120.0)
DB_TIMEOUT_SOCKET_CONNECT=5.0              # Socket connection timeout (1.0-30.0)
DB_TIMEOUT_SOCKET=5.0                       # Socket operation timeout (1.0-60.0)
DB_TIMEOUT_QUERY=60.0                       # Query execution timeout (5.0-300.0)
DB_TIMEOUT_ACQUIRE=2.0                      # Connection pool acquire timeout (0.5-30.0)
DB_TIMEOUT_MEMGRAPH_CONNECTION=30.0         # Memgraph connection timeout (5.0-120.0)
DB_TIMEOUT_MEMGRAPH_COMMAND=60.0            # Memgraph command execution timeout (5.0-300.0)

# Cache (Redis/Valkey) Timeouts
CACHE_TIMEOUT_SOCKET_CONNECT=5.0           # Cache socket connection timeout (1.0-30.0)
CACHE_TIMEOUT_SOCKET=5.0                    # Cache socket operation timeout (1.0-30.0)
CACHE_TIMEOUT_OPERATION=2.0                 # Cache operation timeout (0.5-30.0)

# Async Operation Timeouts
ASYNC_TIMEOUT_QUICK_OPERATION=2.0           # Quick async operations (0.5-30.0)
ASYNC_TIMEOUT_STANDARD_OPERATION=10.0       # Standard async operations (1.0-300.0)
ASYNC_TIMEOUT_LONG_OPERATION=30.0           # Long-running async operations (5.0-600.0)
ASYNC_TIMEOUT_CONSUMER_SHUTDOWN=10.0        # Kafka consumer shutdown (5.0-60.0)
ASYNC_TIMEOUT_EVENT_CONSUMPTION=30.0        # Event consumption task shutdown (10.0-120.0)
ASYNC_TIMEOUT_GIT_OPERATION=5.0             # Git operations (1.0-60.0)
ASYNC_TIMEOUT_GIT_LOG_OPERATION=10.0        # Git log operations (5.0-120.0)

# Background Task Timeouts
BACKGROUND_TIMEOUT_HEALTH_CHECK_INTERVAL=30.0        # Health check interval (5.0-300.0)
BACKGROUND_TIMEOUT_CACHE_CLEANUP_INTERVAL=3600.0     # Cache cleanup interval (60.0-86400.0)
BACKGROUND_TIMEOUT_METRICS_COLLECTION_INTERVAL=300.0 # Metrics collection interval (30.0-3600.0)
BACKGROUND_TIMEOUT_RETRY_BASE_DELAY=1.0              # Base retry delay for exponential backoff (0.1-10.0)
BACKGROUND_TIMEOUT_RETRY_SHORT_DELAY=5.0             # Short retry delay (1.0-60.0)
BACKGROUND_TIMEOUT_RETRY_LONG_DELAY=10.0             # Long retry delay (5.0-120.0)
BACKGROUND_TIMEOUT_PROCESSING_DELAY_SHORT=0.1        # Short processing delay/rate limiting (0.01-5.0)
BACKGROUND_TIMEOUT_PROCESSING_DELAY_MEDIUM=0.5       # Medium processing delay (0.1-10.0)
BACKGROUND_TIMEOUT_PROCESSING_DELAY_LONG=1.0         # Long processing delay (0.5-30.0)

# Test Execution Timeouts
TEST_TIMEOUT_UNIT=60.0                      # Unit tests (10.0-300.0)
TEST_TIMEOUT_INTEGRATION=300.0              # Integration tests - 5 minutes (60.0-1800.0)
TEST_TIMEOUT_E2E=600.0                      # End-to-end tests - 10 minutes (120.0-3600.0)
TEST_TIMEOUT_PERFORMANCE=900.0              # Performance tests - 15 minutes (300.0-3600.0)
TEST_TIMEOUT_PYTEST_TOTAL=1800.0            # Total pytest timeout - 30 minutes (300.0-7200.0)

# Service Restart Timeouts
SERVICE_RESTART_TIMEOUT_DEFAULT=60.0        # Default service restart (10.0-300.0)
SERVICE_RESTART_TIMEOUT_DATABASE=90.0       # Database service restart (30.0-300.0)
SERVICE_RESTART_TIMEOUT_INTELLIGENCE=120.0  # Intelligence service restart (30.0-300.0)
SERVICE_RESTART_TIMEOUT_SEARCH=120.0        # Search service restart (30.0-300.0)
SERVICE_RESTART_TIMEOUT_LANGEXTRACT=180.0   # Langextract service restart (60.0-300.0)
SERVICE_RESTART_TIMEOUT_CONTAINER_STOP=30.0 # Docker container stop (10.0-120.0)
SERVICE_RESTART_TIMEOUT_CONTAINER_WAIT=60.0 # Docker container wait (30.0-300.0)

# Retry Configuration
RETRY_MAX_ATTEMPTS=3                        # Maximum retry attempts (1-10)
RETRY_BACKOFF_MULTIPLIER=2.0               # Exponential backoff multiplier (1.0-5.0)
RETRY_MAX_DELAY=60.0                       # Maximum retry delay in seconds (10.0-300.0)

# ==========================================
# TREE + STAMPING EVENT ADAPTER CONFIGURATION
# ==========================================
# Event-driven integration between OnexTree (file discovery) and
# MetadataStamping (intelligence generation) services.
#
# Purpose: Kafka-based project indexing with batch processing and
# parallel execution for optimal performance (<5min for 1000 files).
#
# Phase 1 (Inline Content) Only: All indexing requires inline content
# provided via bulk_ingest_repository.py. Phase 0 (filesystem-based)
# has been removed. Use bulk_ingest_repository.py to send files with
# content via Kafka events.

# Kafka Topics - Tree Stamping Request Topics (consumed by intelligence service)
KAFKA_TREE_INDEX_PROJECT_REQUEST=dev.archon-intelligence.tree.index-project-requested.v1
KAFKA_TREE_SEARCH_FILES_REQUEST=dev.archon-intelligence.tree.search-files-requested.v1
KAFKA_TREE_GET_STATUS_REQUEST=dev.archon-intelligence.tree.get-status-requested.v1

# Kafka Topics - Tree Stamping Response Topics (published by intelligence service)
KAFKA_TREE_INDEX_PROJECT_COMPLETED=dev.archon-intelligence.tree.index-project-completed.v1
KAFKA_TREE_INDEX_PROJECT_FAILED=dev.archon-intelligence.tree.index-project-failed.v1
KAFKA_TREE_SEARCH_FILES_COMPLETED=dev.archon-intelligence.tree.search-files-completed.v1
KAFKA_TREE_SEARCH_FILES_FAILED=dev.archon-intelligence.tree.search-files-failed.v1
KAFKA_TREE_GET_STATUS_COMPLETED=dev.archon-intelligence.tree.get-status-completed.v1
KAFKA_TREE_GET_STATUS_FAILED=dev.archon-intelligence.tree.get-status-failed.v1

# Tree Stamping Consumer Configuration
TREE_STAMPING_BATCH_SIZE=100                # Files processed per batch (default: 100)
TREE_STAMPING_ENABLE_PARALLEL=true          # Enable parallel batch processing
TREE_STAMPING_MAX_WORKERS=4                 # Max parallel workers for batch processing

# Tree Stamping Concurrency Limits (Rate Limiting)
# Prevents overwhelming external services with too many concurrent requests
INTELLIGENCE_CONCURRENCY_LIMIT=10           # Max concurrent intelligence generation requests (default: 10)
QDRANT_CONCURRENCY_LIMIT=20                 # Max concurrent Qdrant indexing operations (default: 20)
EMBEDDING_CONCURRENCY_LIMIT=100             # Max concurrent OpenAI embedding API calls (default: 100)

# Tree Stamping Service URLs (HTTP clients for TreeStampingBridge orchestrator)
ONEX_TREE_SERVICE_URL=http://archon-tree:8058
METADATA_STAMPING_SERVICE_URL=http://archon-stamping:8057

# ==========================================
# AUTO-INDEXING CONFIGURATION
# ==========================================
# Automatic project indexing on service startup and scheduled re-indexing.
# Makes the tree indexing system fully automatic without manual event publishing.

# Enable/disable automatic indexing (default: true)
AUTO_INDEXING_ENABLED=true

# Index projects on service startup (default: true)
AUTO_INDEX_ON_STARTUP=true

# Watch for file changes and auto-reindex (requires watchdog library, default: false)
AUTO_INDEX_WATCH_CHANGES=false

# Scheduled re-indexing interval in hours (0 = disabled, default: 0)
# Example: 24 = re-index every 24 hours
AUTO_INDEX_SCHEDULE_HOURS=0

# Include test files in indexing (default: true)
AUTO_INDEX_INCLUDE_TESTS=true

# Force re-indexing even if already indexed (default: false)
AUTO_INDEX_FORCE_REINDEX=false

# Option 1: Explicit project list (comma-separated absolute paths)
# Example: AUTO_INDEX_PROJECTS=/Volumes/PRO-G40/Code/omniarchon,/Users/jonah/Code/myproject
AUTO_INDEX_PROJECTS=/Volumes/PRO-G40/Code/omniarchon

# Option 2: Workspace directory to scan for projects (alternative to explicit list)
# Service will auto-discover projects with .git, pyproject.toml, package.json, etc.
# Example: AUTO_INDEX_WORKSPACE=/Users/jonah/Code
# AUTO_INDEX_WORKSPACE=

# Auto-Indexing Usage Examples:
#
# Production (index omniarchon on startup):
# AUTO_INDEXING_ENABLED=true
# AUTO_INDEX_ON_STARTUP=true
# AUTO_INDEX_PROJECTS=/Volumes/PRO-G40/Code/omniarchon
#
# Development (watch workspace and re-index on changes):
# AUTO_INDEXING_ENABLED=true
# AUTO_INDEX_WATCH_CHANGES=true
# AUTO_INDEX_WORKSPACE=/Users/jonah/Code
#
# Scheduled re-indexing (daily):
# AUTO_INDEXING_ENABLED=true
# AUTO_INDEX_SCHEDULE_HOURS=24
# AUTO_INDEX_FORCE_REINDEX=true
#
# Disable auto-indexing (manual event publishing):
# AUTO_INDEXING_ENABLED=false

# Caching & Gateway Defaults
# Required by coding guidelines
ARCHON_ENABLE_EXTERNAL_GATEWAY=false
ENABLE_CACHE=true
VALKEY_URL=redis://archon-valkey:6379/0
VALKEY_PASSWORD=YOUR_VALKEY_PASSWORD  # Generate with: openssl rand -hex 16
VALKEY_LOGLEVEL=notice  # Options: debug, verbose, notice, warning, nothing (default: notice)

# ==========================================
# KAFKA/REDPANDA BROKER CONFIGURATION
# ==========================================
# IMPORTANT: Use correct addresses based on execution context:
#
# Docker Containers (services/intelligence/app.py):
#   - Use internal service name: omninode-bridge-redpanda:9092
#   - Example: KAFKA_BOOTSTRAP_SERVERS=omninode-bridge-redpanda:9092
#
# Host Machine Scripts (scripts/*.py):
#   - Use external mapped port: localhost:29092 (or network-specific IP if remote)
#   - Example: KAFKA_BOOTSTRAP_SERVERS=localhost:29092
#
# Why different addresses?
#   - Docker containers use internal DNS (service names)
#   - Host scripts use external network (IPs/localhost with mapped ports)
#
# Override any default with this environment variable to avoid restarts!
#
KAFKA_BOOTSTRAP_SERVERS=omninode-bridge-redpanda:9092

# ==========================================
# BULK INGESTION DEFAULTS
# ==========================================
# Default configuration for bulk_ingest_repository.py script.
# These values control batch processing and file size limits for
# repository indexing via Kafka events.

# Default batch size for bulk repository ingestion (files per batch)
# Controls how many files are sent per Kafka message batch
# Range: 1-1000, recommended: 10-100 for optimal memory usage
BULK_INGEST_BATCH_SIZE=10

# Default maximum file size for bulk ingestion (bytes)
# Files larger than this will be skipped with a warning
# 5242880 = 5MB - prevents memory issues with large binary files
# Range: 1024-104857600 (1KB-100MB), recommended: 1MB-10MB
BULK_INGEST_MAX_FILE_SIZE=5242880

# Rate limiting for bulk ingestion (events per second)
# Controls how fast events are published to prevent overwhelming archon-intelligence
# Lower values = less CPU/memory pressure on intelligence service
# Higher values = faster ingestion but may cause resource spikes
# Range: 1-100, recommended: 20-30 for production, 50+ for local dev
# Set to 0 to disable rate limiting (use with caution!)
BULK_INGEST_RATE_LIMIT=25

# ==========================================
# INGESTION PIPELINE CONFIGURATION
# ==========================================
# Advanced ingestion pipeline configuration for handling large files
# and preventing Kafka message size errors during bulk operations.
#
# OVERVIEW:
# These settings work together to prevent Kafka message size errors
# while maintaining data integrity during repository ingestion.
#
# PROBLEM SOLVED:
# - Kafka has a default 5MB message size limit (configured via KAFKA_MAX_REQUEST_SIZE)
# - Large files or large batches can exceed this limit, causing ingestion failures
# - These settings provide automatic splitting and filtering to prevent errors
#
# HOW IT WORKS:
# 1. Binary files are automatically excluded (images, fonts, compiled files, etc.)
# 2. Files >MAX_FILE_SIZE_MB use path-only strategy (content not included inline)
# 3. Batches >BATCH_SIZE_THRESHOLD_MB are automatically split before sending to Kafka
# 4. BATCH_SIZE_THRESHOLD_MB is set to 90% of KAFKA_MAX_REQUEST_SIZE for safety margin

# Maximum file size for inline content inclusion (MB)
# Files larger than this will use path-only strategy (no inline content)
# This prevents individual large files from bloating Kafka messages
# Default: 2.0 MB (covers most source code files)
# Range: 0.1-10.0 MB
# Impact: Larger values = more files indexed with full content, but higher memory usage
MAX_FILE_SIZE_MB=2.0

# Maximum batch size before automatic splitting (MB)
# Batches exceeding this will be split to prevent Kafka message size errors
# Default: 4.5 MB (90% of 5MB Kafka limit to provide safety margin)
# Range: 1.0-4.8 MB (must be less than KAFKA_MAX_REQUEST_SIZE)
# Impact: Smaller values = more batches, slower ingestion but safer
BATCH_SIZE_THRESHOLD_MB=4.5

# Kafka maximum message size (bytes)
# Should match your Kafka broker's message.max.bytes configuration
# Default: 5242880 (5MB) - standard Kafka/Redpanda limit
# Range: 1048576-104857600 (1MB-100MB)
# WARNING: Changing this requires updating Kafka broker configuration
# Impact: Higher values allow larger batches but may cause broker issues
KAFKA_MAX_REQUEST_SIZE=5242880

# TUNING GUIDANCE:
#
# Increase MAX_FILE_SIZE_MB when:
# - You need larger files (e.g., generated code, data files) indexed with full content
# - Your Kafka limit is higher than 5MB
# - You have plenty of memory and fast network
#
# Decrease BATCH_SIZE_THRESHOLD_MB when:
# - Still encountering "MessageSizeTooLargeError" errors
# - Working with many large files (even below MAX_FILE_SIZE_MB)
# - Conservative approach preferred
#
# Increase KAFKA_MAX_REQUEST_SIZE when:
# - You control the Kafka broker and can change message.max.bytes
# - Processing very large codebases with big files
# - WARNING: Must update Kafka broker config FIRST
#
# RECOMMENDED SETTINGS BY USE CASE:
#
# Standard Development (default):
#   MAX_FILE_SIZE_MB=2.0
#   BATCH_SIZE_THRESHOLD_MB=4.5
#   KAFKA_MAX_REQUEST_SIZE=5242880
#
# Large Codebases (more conservative):
#   MAX_FILE_SIZE_MB=1.0
#   BATCH_SIZE_THRESHOLD_MB=3.5
#   KAFKA_MAX_REQUEST_SIZE=5242880
#
# High-Performance Setup (requires Kafka config change):
#   MAX_FILE_SIZE_MB=5.0
#   BATCH_SIZE_THRESHOLD_MB=9.0
#   KAFKA_MAX_REQUEST_SIZE=10485760  # 10MB
#   # Also set in Kafka broker: message.max.bytes=10485760
#
# VALIDATION RULE:
# BATCH_SIZE_THRESHOLD_MB should always be 85-95% of (KAFKA_MAX_REQUEST_SIZE / 1048576)
# Example: 5242880 bytes = 5.0 MB ‚Üí 90% = 4.5 MB threshold

# ==========================================
# DISTRIBUTED DEPLOYMENT CONFIGURATION
# ==========================================
# For distributed deployments, update the following services to point to remote hosts:
# Example: OmniNode Bridge stack running on a remote server (e.g., 192.168.x.x)
#
# POSTGRES_HOST=<remote_ip>
# DATABASE_URL=postgresql://postgres:YOUR_POSTGRES_PASSWORD@<remote_ip>:5432/omninode_bridge
# MEMGRAPH_URI=bolt://<remote_ip>:7687
# VALKEY_URL=redis://:YOUR_VALKEY_PASSWORD@<remote_ip>:6379/0
# KAFKA_BOOTSTRAP_SERVERS=<remote_ip>:29092  # Use external port for host machine
# BRIDGE_SERVICE_URL=http://<remote_ip>:8054
#
# This demonstrates Archon's distributed architecture capabilities where heavy
# infrastructure (PostgreSQL, Memgraph, Kafka, Valkey cache) can run on separate
# servers while Archon intelligence services run locally.

# External Service Configuration
# OLLAMA_BASE_URL: Ollama API endpoint (defaults to host.docker.internal:11434 in Docker)
OLLAMA_BASE_URL=http://192.168.86.200:11434

# ==========================================
# MULTI-MACHINE EMBEDDING CONFIGURATION
# ==========================================
# Distributed embedding endpoints for parallel processing
# Option A Implementation: Multiple consumer instances with dedicated GPU machines
#
# Supported Backends:
#   - vLLM: OpenAI-compatible embedding server (port 8002, recommended)
#
# Architecture:
#   Consumer 1 ‚Üí GPU Machine 1 (vLLM on 4090)   ‚Üí 192.168.86.201:8002
#   Consumer 2 ‚Üí GPU Machine 1 (vLLM on 4090)   ‚Üí 192.168.86.201:8002
#   Consumer 3 ‚Üí GPU Machine 2 (vLLM on 5090)   ‚Üí 192.168.86.202:8002
#   Consumer 4 ‚Üí GPU Machine 1 (round-robin)    ‚Üí 192.168.86.201:8002 (default)
#
# Benefits:
#   - 3x throughput (3 embedding instances)
#   - ~85% success rate ‚Üí 100% (eliminates PoolTimeout errors)
#   - Processing time: 6 minutes ‚Üí 2 minutes
#
# ==========================================
# vLLM Backend Setup (Recommended)
# ==========================================
# Setup Requirements:
#   1. Deploy vLLM cluster:
#      ./scripts/deploy_vllm_embedding_cluster.sh
#   2. Or manually on each machine:
#      python -m vllm.entrypoints.openai.api_server \
#        --model rjmalagon/gte-qwen2-1.5b-instruct-embed-f16 \
#        --port 8002 --max-model-len 8192
#   3. Verify connectivity:
#      curl http://192.168.86.201:8002/v1/models
#      curl http://192.168.86.202:8002/v1/models
#
# Consumer-specific embedding endpoints (vLLM)
EMBEDDING_BASE_URL_CONSUMER_2=http://192.168.86.201:8002  # vLLM GPU Machine 1 (4090)
EMBEDDING_BASE_URL_CONSUMER_3=http://192.168.86.202:8002  # vLLM GPU Machine 2 (5090)
EMBEDDING_BASE_URL_CONSUMER_4=http://192.168.86.201:8002  # Default vLLM endpoint

# PostgreSQL Database Password (for document freshness and pattern traceability)
# ‚ö†Ô∏è SECURITY: Get actual password from remote server admin
POSTGRES_PASSWORD=YOUR_POSTGRES_PASSWORD  # Contact admin for actual password

# ==========================================
# CONSUL SERVICE DISCOVERY CONFIGURATION
# ==========================================
# Consul service discovery for distributed deployments
# Enables automatic service registration and health monitoring
#
# Consul server location (typically on infrastructure host)
CONSUL_HOST=192.168.86.200
CONSUL_PORT=8500
#
# Enable/disable Consul integration (default: true for production)
CONSUL_ENABLED=true
#
# Usage Examples:
#
# Development (local services, no Consul):
# CONSUL_ENABLED=false
#
# Production (distributed deployment with service discovery):
# CONSUL_HOST=consul.example.com
# CONSUL_PORT=8500
# CONSUL_ENABLED=true
#
# Consul provides:
# - Automatic service registration and health checks
# - Dynamic service discovery (find services by name)
# - Health monitoring and failure detection
# - Distributed configuration management
#
# Access Consul UI: http://${CONSUL_HOST}:${CONSUL_PORT}/ui

# ==========================================
# NOTE: All other configuration has been moved to database management!
# Then use the Settings page in the web UI to manage:
# - OPENAI_API_KEY (encrypted)
# - MODEL_CHOICE
# - TRANSPORT settings
# - RAG strategy flags (USE_CONTEXTUAL_EMBEDDINGS, USE_HYBRID_SEARCH, etc.)
# - Crawler settings:
#   * CRAWL_MAX_CONCURRENT (default: 10) - Max concurrent pages per crawl operation
#   * CRAWL_BATCH_SIZE (default: 50) - URLs processed per batch
#   * MEMORY_THRESHOLD_PERCENT (default: 80) - Memory % before throttling
#   * DISPATCHER_CHECK_INTERVAL (default: 0.5) - Memory check interval in seconds

# ==========================================
# SLACK ALERTING CONFIGURATION
# ==========================================
# Comprehensive alerting for Archon container infrastructure with intelligent throttling
# Documentation: docs/SLACK_ALERTING.md
#
# Quick Start:
# 1. Create Slack incoming webhook: https://api.slack.com/messaging/webhooks
# 2. Set ALERT_NOTIFICATION_SLACK_WEBHOOK_URL below
# 3. Run: python scripts/slack_alerting.py --daemon
#
# Features:
# - Monitor all Archon containers (bridge, intelligence, search, consumers, databases)
# - Detect crashes, restarts, health failures, high resource usage, consumer lag
# - Intelligent throttling: aggregate errors, rate limit, prevent alert flooding
# - Escalation for repeated failures
# - Recovery notifications
# - State persistence across restarts

# Slack Webhook URL (REQUIRED for alerting)
# Get from: Slack Workspace ‚Üí Apps ‚Üí Incoming Webhooks ‚Üí Add New Webhook to Workspace
ALERT_NOTIFICATION_SLACK_WEBHOOK_URL=

# Alert Thresholds
ALERT_THRESHOLD_CONTAINER_RESTART_COUNT=3           # Restarts before critical alert
ALERT_THRESHOLD_CPU_PERCENT_WARNING=80.0            # CPU % for warning
ALERT_THRESHOLD_CPU_PERCENT_CRITICAL=95.0           # CPU % for critical
ALERT_THRESHOLD_MEMORY_MB_WARNING=3072.0            # Memory MB for warning (3GB)
ALERT_THRESHOLD_MEMORY_MB_CRITICAL=3686.4           # Memory MB for critical (3.6GB)
ALERT_THRESHOLD_CONSUMER_LAG_WARNING=100            # Consumer lag for warning
ALERT_THRESHOLD_CONSUMER_LAG_CRITICAL=500           # Consumer lag for critical
ALERT_THRESHOLD_ERROR_RATE_WARNING=10               # Errors in 5min for warning
ALERT_THRESHOLD_ERROR_RATE_CRITICAL=50              # Errors in 5min for critical
ALERT_THRESHOLD_HEALTH_CHECK_TIMEOUT_SECONDS=10.0   # Health check timeout
ALERT_THRESHOLD_CONSECUTIVE_HEALTH_FAILURES=3       # Failures before alert

# Throttling Configuration (prevents alert flooding)
ALERT_THROTTLE_RATE_LIMIT_WINDOW_SECONDS=300        # Rate limit window (5 minutes)
ALERT_THROTTLE_MAX_ALERTS_PER_WINDOW=1              # Max alerts per window per service per type
ALERT_THROTTLE_ERROR_AGGREGATION_WINDOW_SECONDS=300 # Error aggregation window (5 minutes)
ALERT_THROTTLE_MIN_ERRORS_FOR_AGGREGATION=10        # Min errors before aggregated alert
ALERT_THROTTLE_ESCALATION_THRESHOLD=3               # Consecutive failures to escalate
ALERT_THROTTLE_ESCALATION_MULTIPLIER=2.0            # Escalation severity multiplier
ALERT_THROTTLE_RECOVERY_COOLDOWN_SECONDS=300        # Cooldown after recovery (5 minutes)
ALERT_THROTTLE_DEDUPLICATION_WINDOW_SECONDS=60      # Duplicate alert suppression window

# Notification Settings
ALERT_NOTIFICATION_INCLUDE_METRICS=true             # Include detailed metrics in alerts
ALERT_NOTIFICATION_INCLUDE_RECOVERY_ALERTS=true     # Send recovery notifications
ALERT_NOTIFICATION_EMOJI_CRITICAL=üö®               # Critical alert emoji
ALERT_NOTIFICATION_EMOJI_WARNING=‚ö†Ô∏è                # Warning alert emoji
ALERT_NOTIFICATION_EMOJI_INFO=‚ÑπÔ∏è                   # Info alert emoji
ALERT_NOTIFICATION_EMOJI_RECOVERY=‚úÖ               # Recovery alert emoji
ALERT_NOTIFICATION_ALERT_PREFIX=[Archon Alert]      # Alert message prefix

# Monitoring Loop
MONITORING_CHECK_INTERVAL_SECONDS=30                # Main monitoring interval
MONITORING_HEALTH_CHECK_INTERVAL_SECONDS=60         # Health check interval
MONITORING_METRICS_COLLECTION_INTERVAL_SECONDS=120  # Metrics collection interval
MONITORING_STATE_FILE_PATH=/tmp/archon_alerting_state.json  # State persistence
MONITORING_MAX_HISTORY_ENTRIES=1000                 # Max alert history size
MONITORING_LOG_LEVEL=INFO                           # Logging level (DEBUG, INFO, WARNING, ERROR)

# Usage Examples:
#
# Test configuration (one-shot check):
# python scripts/slack_alerting.py --webhook $ALERT_NOTIFICATION_SLACK_WEBHOOK_URL
#
# Production daemon mode:
# python scripts/slack_alerting.py --daemon
#
# Custom interval (check every 60 seconds):
# python scripts/slack_alerting.py --daemon --interval 60
#
# Verbose logging for debugging:
# python scripts/slack_alerting.py --daemon --verbose

# =====================================================
# PERFORMANCE TUNING CONFIGURATION (2025-11-06)
# =====================================================
# Connection pool and concurrency limits to prevent resource exhaustion
# during bulk ingestion operations

# Memgraph Connection Pool Configuration
MEMGRAPH_MAX_CONNECTIONS=50
MEMGRAPH_CONNECTION_TIMEOUT=120
# Concurrency throttling to prevent TransientError storms during bulk ingestion
# Lower values (5-10) reduce conflicts but increase latency
# Higher values (15-20) improve throughput but may cause more retries
MEMGRAPH_MAX_CONCURRENT_WRITES=10

# Consumer Concurrency Configuration
# Reduced from 8 to 4 workers to limit concurrent Memgraph connections
CONSUMER_MAX_WORKERS=4
CONSUMER_BATCH_SIZE=10

# Bulk Ingestion Rate Limiting
# Reduce concurrent batches and add delay to prevent overwhelming services
INGESTION_MAX_CONCURRENT_BATCHES=2
INGESTION_BATCH_DELAY_MS=500
